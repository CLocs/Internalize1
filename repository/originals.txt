Foreword
BY SHERYL SANDBERG
Chief operating officer of Facebook and founder of LeanIn.Org
Adam Grant is the perfect person to write Originals because he is one.
He is a brilliant researcher who passionately pursues the science of what motivates people, busting myths and revealing truths. He is an informed optimist who offers insights and advice about how anyone—at home, at work, in the community—can make the world a better place. He is a dedicated friend who inspires me to believe in myself and has helped me understand how I can advocate effectively for my ideas.
Adam is one of the most important influences in my life. Through the pages of this magnificent book, he will enlighten, inspire, and support you as well.
MYTH BUSTER
Conventional wisdom holds that some people are innately creative, while most have few original thoughts. Some people are born to be leaders, and the rest are followers. Some people can have real impact, but the majority can’t.
In Originals Adam shatters all of these assumptions.
He demonstrates that any of us can enhance our creativity. He reveals how we can identify ideas that are truly original and predict which ones will work. He tells us when to trust our gut and when to rely on others. He shows how we can become better parents by nurturing originality in our children and better managers by fostering diversity of thought instead of conformity.
In these pages, I learned that great creators don’t necessarily have the deepest expertise but rather seek out the broadest perspectives. I saw how success is not usually attained by being ahead of everyone else but by waiting patiently for the right time to act. And to my utter shock, I learned that procrastinating can be good. Anyone who has ever worked with me knows how much I hate leaving things to the last minute, how I always think that anything that can be done should be done right away. Mark Zuckerberg, along with many others, will be pleased if I can let go of the relentless pressure I feel to finish everything early—and, as Adam points out, it might just help me and my teams achieve better results.
INFORMED OPTIMIST
Every day, we all encounter things we love and things that need to change. The former give us joy. The latter fuel our desire to make the world different—ideally better than the way we found it. But trying to change deep-seated beliefs and behaviors is daunting. We accept the status quo because effecting real change seems impossible. Still, we dare to ask: Can one individual make a difference? And, in our bravest moments: Could that one individual be me?
Adam’s answer is a resounding yes. This book proves that any one of us can champion ideas that improve the world around us.
FRIEND
I met Adam just as his first book, Give and Take, was generating buzz in Silicon Valley. I read it and immediately started quoting it to anyone who would listen. Adam was not only a talented researcher but also a gifted teacher and storyteller who was able to explain complicated ideas simply and clearly.
Then my husband invited Adam to speak to his team at work and brought him over for dinner. Adam was every bit as extraordinary in person as he was on paper. His knowledge was encyclopedic and his energy was contagious. He and I started talking about how his research could inform the debate on gender and began working together. We have done so ever since, conducting research and writing a series of op-eds about women and work. LeanIn.Org has benefited immensely from his rigorous analysis and commitment to equality.
Once a year, Facebook brings its global teams together, and in 2015 I invited Adam to give a keynote speech. Everyone was blown away by his wisdom and humor. Months later, the teams are still talking about his insights and putting his advice into action.
Along the way, Adam and I became friends. When tragedy hit and I lost my husband suddenly, Adam stepped up and stepped in as only a true friend would. He approached the worst time of my life as he approaches everything, combining his unique understanding of psychology with his unparalleled generosity. When I thought I would never feel better, he flew across the country to explain what I could do to build my resilience. When I could not figure out how to handle a particularly gut-wrenching situation, he helped me find answers where I thought there were none. When I needed a shoulder to cry on, his was always there.
In the deepest sense of the word, a friend is someone who sees more potential in you than you see in yourself, someone who helps you become the best version of yourself. The magic of this book is that Adam becomes that kind of friend to everyone who reads it. He offers a wealth of advice for overcoming doubt and fear, speaking up and pitching ideas, and finding allies in the least likely of places. He gives practical guidance on how to manage anxiety, channel anger, find the strength in our weaknesses, overcome obstacles, and give hope to others.
—
Originals is one of the most important and captivating books I have ever read, full of surprising and powerful ideas. It will not only change the way you see the world; it might just change the way you live your life. And it could very well inspire you to change your world.
1
Creative Destruction
The Risky Business of Going Against the Grain
	“The reasonable man adapts himself to the world; the unreasonable one persists in trying to adapt the world to himself. Therefore all progress depends on the unreasonable man.”
	George Bernard Shaw
	O
n a cool fall evening in 2008, four students set out to revolutionize an industry. Buried in loans, they had lost and broken eyeglasses and were outraged at how much it cost to replace them. One of them had been wearing the same damaged pair for five years: He was using a paper clip to bind the frames together. Even after his prescription changed twice, he refused to pay for pricey new lenses.
Luxottica, the 800-pound gorilla of the industry, controlled more than 80 percent of the eyewear market. To make glasses more affordable, the students would need to topple a giant. Having recently watched Zappos transform footwear by selling shoes online, they wondered if they could do the same with eyewear.
When they casually mentioned their idea to friends, time and again they were blasted with scorching criticism. No one would ever buy glasses over the internet, their friends insisted. People had to try them on first. Sure, Zappos had pulled the concept off with shoes, but there was a reason it hadn’t happened with eyewear. “If this were a good idea,” they heard repeatedly, “someone would have done it already.”
None of the students had a background in e-commerce and technology, let alone in retail, fashion, or apparel. Despite being told their idea was crazy, they walked away from lucrative job offers to start a company. They would sell eyeglasses that normally cost $500 in a store for $95 online, donating a pair to someone in the developing world with every purchase.
The business depended on a functioning website. Without one, it would be impossible for customers to view or buy their products. After scrambling to pull a website together, they finally managed to get it online at 4 A.M. on the day before the launch in February 2010. They called the company Warby Parker, combining the names of two characters created by the novelist Jack Kerouac, who inspired them to break free from the shackles of social pressure and embark on their adventure. They admired his rebellious spirit, infusing it into their culture. And it paid off.
The students expected to sell a pair or two of glasses per day. But when GQ called them “the Netflix of eyewear,” they hit their target for the entire first year in less than a month, selling out so fast that they had to put twenty thousand customers on a waiting list. It took them nine months to stock enough inventory to meet the demand.
Fast forward to 2015, when Fast Company released a list of the world’s most innovative companies. Warby Parker didn’t just make the list—they came in first. The three previous winners were creative giants Google, Nike, and Apple, all with over fifty thousand employees. Warby Parker’s scrappy startup, a new kid on the block, had a staff of just five hundred. In the span of five years, the four friends built one of the most fashionable brands on the planet and donated over a million pairs of glasses to people in need. The company cleared $100 million in annual revenues and was valued at over $1 billion.
Back in 2009, one of the founders pitched the company to me, offering me the chance to invest in Warby Parker. I declined.
It was the worst financial decision I’ve ever made, and I needed to understand where I went wrong.
—
	orig•i•nal, adj The origin or source of something; from which something springs, proceeds, or is derived.
	orig•i•nal, n A thing of singular or unique character; a person who is different from other people in an appealing or interesting way; a person of fresh initiative or inventive capacity.
Years ago, psychologists discovered that there are two routes to achievement: conformity and originality. Conformity means following the crowd down conventional paths and maintaining the status quo. Originality is taking the road less traveled, championing a set of novel ideas that go against the grain but ultimately make things better.
Of course, nothing is completely original, in the sense that all of our ideas are influenced by what we learn from the world around us. We are constantly borrowing thoughts, whether intentionally or inadvertently. We’re all vulnerable to “kleptomnesia”—accidentally remembering the ideas of others as our own. By my definition, originality involves introducing and advancing an idea that’s relatively unusual within a particular domain, and that has the potential to improve it.
Originality itself starts with creativity: generating a concept that is both novel and useful. But it doesn’t stop there. Originals are people who take the initiative to make their visions a reality. The Warby Parker founders had the originality to dream up an unconventional way to sell glasses online, but became originals by taking action to make them easily accessible and affordable.
This book is about how we can all become more original. There’s a surprising clue in the web browser that you use to surf the internet.
Finding the Faults in Defaults
Not long ago, economist Michael Housman was leading a project to figure out why some customer service agents stayed in their jobs longer than others. Armed with data from over thirty thousand employees who handled calls for banks, airlines, and cell-phone companies, he suspected that their employment histories would contain telltale signs about their commitment. He thought that people with a history of job-hopping would quit sooner, but they didn’t: Employees who had held five jobs in the past five years weren’t any more likely to leave their positions than those who had stayed in the same job for five years.
Hunting for other hints, he noticed that his team had captured information about which internet browser employees had used when they logged in to apply for their jobs. On a whim, he tested whether that choice might be related to quitting. He didn’t expect to find any correlation, assuming that browser preference was purely a matter of taste. But when he looked at the results, he was stunned: Employees who used Firefox or Chrome to browse the Web remained in their jobs 15 percent longer than those who used Internet Explorer or Safari.
Thinking it was a coincidence, Housman ran the same analysis for absences from work. The pattern was the same: Firefox and Chrome users were 19 percent less likely to miss work than Internet Explorer and Safari fans.
Then he looked at performance. His team had assembled nearly three million data points on sales, customer satisfaction, and average call length. The Firefox and Chrome users had significantly higher sales, and their call times were shorter. Their customers were happier, too: After 90 days on the job, the Firefox and Chrome users had customer satisfaction levels that Internet Explorer and Safari users reached only after 120 days at work.
It’s not the browser itself that’s causing them to stick around, show up dependably, and succeed. Rather, it’s what their browser preference signals about their habits. Why are the Firefox and Chrome users more committed and better performers on every metric?
The obvious answer was that they’re more tech savvy, so I asked Housman if he could explore that. The employees had all taken a computer proficiency test, which assessed their knowledge of keyboard shortcuts, software programs, and hardware, as well as a timed test of their typing speed. But the Firefox and Chrome group didn’t prove to have significantly more computer expertise, and they weren’t faster or more accurate typists. Even after accounting for those scores, the browser effect persisted. Technical knowledge and skill weren’t the source of their advantage.
What made the difference was how they obtained the browser. If you own a PC, Internet Explorer is built into Windows. If you’re a Mac user, your computer came preinstalled with Safari. Almost two thirds of the customer service agents used the default browser, never questioning whether a better one was available.
To get Firefox or Chrome, you have to demonstrate some resourcefulness and download a different browser. Instead of accepting the default, you take a bit of initiative to seek out an option that might be better. And that act of initiative, however tiny, is a window into what you do at work.
The customer service agents who accepted the defaults of Internet Explorer and Safari approached their job the same way. They stayed on script in sales calls and followed standard operating procedures for handling customer complaints. They saw their job descriptions as fixed, so when they were unhappy with their work, they started missing days, and eventually just quit.
The employees who took the initiative to change their browsers to Firefox or Chrome approached their jobs differently. They looked for novel ways of selling to customers and addressing their concerns. When they encountered a situation they didn’t like, they fixed it. Having taken the initiative to improve their circumstances, they had little reason to leave. They created the jobs they wanted. But they were the exception, not the rule.
We live in an Internet Explorer world. Just as almost two thirds of the customer service reps used the default browser on their computers, many of us accept the defaults in our own lives. In a series of provocative studies, a team led by political psychologist John Jost explored how people responded to undesirable default conditions. Compared to European Americans, African Americans were less satisfied with their economic circumstances but perceived economic inequality as more legitimate and just. Compared to people in the highest income bracket, people in the lowest income bracket were 17 percent more likely to view economic inequality as necessary. And when asked whether they would support laws that limit the rights of citizens and the press to criticize the government if enacting such legislation was necessary to solve our nation’s problems, twice as many people in the lowest income bracket were willing to give up the right to free speech as those in the highest income bracket. After finding that disadvantaged groups consistently support the status quo more than advantaged groups, Jost and his colleagues concluded: “People who suffer the most from a given state of affairs are paradoxically the least likely to question, challenge, reject, or change it.”
To explain this peculiar phenomenon, Jost’s team developed a theory of system justification. Its core idea is that people are motivated to rationalize the status quo as legitimate—even if it goes directly against their interests. In one study, they tracked Democratic and Republican voters before the 2000 U.S. presidential election. When George W. Bush gained in the polls, Republicans rated him as more desirable, but so did Democrats, who were already preparing justifications for the anticipated status quo. The same happened when Al Gore’s likelihood of success increased: Both Republicans and Democrats judged him more favorably. Regardless of political ideologies, when a candidate seemed destined to win, people liked him more. When his odds dropped, they liked him less.
Justifying the default system serves a soothing function. It’s an emotional painkiller: If the world is supposed to be this way, we don’t need to be dissatisfied with it. But acquiescence also robs us of the moral outrage to stand against injustice and the creative will to consider alternative ways that the world could work.
—
The hallmark of originality is rejecting the default and exploring whether a better option exists. I’ve spent more than a decade studying this, and it turns out to be far less difficult than I expected.
The starting point is curiosity: pondering why the default exists in the first place. We’re driven to question defaults when we experience vuja de, the opposite of déjà vu. Déjà vu occurs when we encounter something new, but it feels as if we’ve seen it before. Vuja de is the reverse—we face something familiar, but we see it with a fresh perspective that enables us to gain new insights into old problems.
Without a vuja de event, Warby Parker wouldn’t have existed. When the founders were sitting in the computer lab on the night they conjured up the company, they had spent a combined sixty years wearing glasses. The product had always been unreasonably expensive. But until that moment, they had taken the status quo for granted, never questioning the default price. “The thought had never crossed my mind,” cofounder Dave Gilboa says. “I had always considered them a medical purchase. I naturally assumed that if a doctor was selling it to me, there was some justification for the price.”
Having recently waited in line at the Apple Store to buy an iPhone, he found himself comparing the two products. Glasses had been a staple of human life for nearly a thousand years, and they’d hardly changed since his grandfather wore them. For the first time, Dave wondered why glasses had such a hefty price tag. Why did such a fundamentally simple product cost more than a complex smartphone?
Anyone could have asked those questions and arrived at the same answer that the Warby Parker squad did. Once they became curious about why the price was so steep, they began doing some research on the eyewear industry. That’s when they learned that it was dominated by Luxottica, a European company that had raked in over $7 billion the previous year. “Understanding that the same company owned LensCrafters and Pearle Vision, Ray-Ban and Oakley, and the licenses for Chanel and Prada prescription frames and sunglasses—all of a sudden, it made sense to me why glasses were so expensive,” Dave says. “Nothing in the cost of goods justified the price.” Taking advantage of its monopoly status, Luxottica was charging twenty times the cost. The default wasn’t inherently legitimate; it was a choice made by a group of people at a given company. And this meant that another group of people could make an alternative choice. “We could do things differently,” Dave suddenly understood. “It was a realization that we could control our own destiny, that we could control our own prices.”
When we become curious about the dissatisfying defaults in our world, we begin to recognize that most of them have social origins: Rules and systems were created by people. And that awareness gives us the courage to contemplate how we can change them. Before women gained the right to vote in America, many “had never before considered their degraded status as anything but natural,” historian Jean Baker observes. As the suffrage movement gained momentum, “a growing number of women were beginning to see that custom, religious precept, and law were in fact man-made and therefore reversible.”
The Two Faces of Ambition
The pressures to accept defaults start much earlier than we realize. If you consider the individuals who will grow up and make a dent in the universe, the first group that probably comes to mind is child prodigies. These geniuses learn to read at age two, play Bach at four, breeze through calculus at six, and speak seven languages fluently by eight. Their classmates shudder with jealousy; their parents rejoice at having won the lottery. But to paraphrase T. S. Eliot, their careers tend to end not with a bang, but a whimper.
Child prodigies, it turns out, rarely go on to change the world. When psychologists study history’s most eminent and influential people, they discover that many of them weren’t unusually gifted as children. And if you assemble a large group of child prodigies and follow them for their entire lives, you’ll find that they don’t outshine their less precocious peers from families of similar means.
Intuitively, this makes sense. We assume that what gifted kids have in book smarts, they lack in street smarts. While they have the intellectual chops, they must lack the social, emotional, and practical skills to function in society. When you look at the evidence, though, this explanation falls short: Less than a quarter of gifted children suffer from social and emotional problems. The vast majority are well-adjusted—as delightful at a cocktail party as in a spelling bee.
Although child prodigies are often rich in both talent and ambition, what holds them back from moving the world forward is that they don’t learn to be original. As they perform in Carnegie Hall, win the science Olympics, and become chess champions, something tragic happens: Practice makes perfect, but it doesn’t make new. The gifted learn to play magnificent Mozart melodies and beautiful Beethoven symphonies, but never compose their own original scores. They focus their energy on consuming existing scientific knowledge, not producing new insights. They conform to the codified rules of established games, rather than inventing their own rules or their own games. All along the way, they strive to earn the approval of their parents and the admiration of their teachers.
Research demonstrates that it is the most creative children who are the least likely to become the teacher’s pet. In one study, elementary school teachers listed their favorite and least favorite students, and then rated both groups on a list of characteristics. The least favorite students were the non-conformists who made up their own rules. Teachers tend to discriminate against highly creative students, labeling them as troublemakers. In response, many children quickly learn to get with the program, keeping their original ideas to themselves. In the language of author William Deresiewicz, they become the world’s most excellent sheep.
In adulthood, many child prodigies become experts in their fields and leaders in their organizations. Yet “only a fraction of gifted children eventually become revolutionary adult creators,” laments psychologist Ellen Winner. “Those who do must make a painful transition” from a child who “learns rapidly and effortlessly in an established domain” to an adult who “ultimately remakes a domain.”
Most prodigies never make that leap. They apply their extraordinary abilities in ordinary ways, mastering their jobs without questioning defaults and without making waves. In every domain they enter, they play it safe by following the conventional paths to success. They become doctors who heal their patients without fighting to fix the broken systems that prevent many patients from affording health care in the first place. They become lawyers who defend clients for violating outdated laws without trying to transform the laws themselves. They become teachers who plan engaging algebra lessons without questioning whether algebra is what their students need to learn. Although we rely on them to keep the world running smoothly, they keep us running on a treadmill.
Child prodigies are hindered by achievement motivation. The drive to succeed is responsible for many of the world’s greatest accomplishments. When we’re determined to excel, we have the fuel to work harder, longer, and smarter. But as cultures rack up a significant number of achievements, originality is increasingly left to a specialized few.
When achievement motivation goes sky-high, it can crowd out originality: The more you value achievement, the more you come to dread failure. Instead of aiming for unique accomplishments, the intense desire to succeed leads us to strive for guaranteed success. As psychologists Todd Lubart and Robert Sternberg put it, “Once people pass an intermediate level in the need to achieve, there is evidence that they actually become less creative.”
The drive to succeed and the accompanying fear of failure have held back some of the greatest creators and change agents in history. Concerned with maintaining stability and attaining conventional achievements, they have been reluctant to pursue originality. Instead of charging full steam ahead with assurance, they have been coaxed, convinced, or coerced to take a stand. While they may seem to have possessed the qualities of natural leaders, they were figuratively—and sometimes literally—lifted up by followers and peers. If a handful of people hadn’t been cajoled into taking original action, America might not exist, the civil rights movement could still be a dream, the Sistine Chapel might be bare, we might still believe the sun revolves around the earth, and the personal computer might never have been popularized.
From our perspective today, the Declaration of Independence seems inevitable, but it nearly didn’t happen due to the reluctance of key revolutionaries. “The men who took commanding roles in the American Revolution were as unlikely a group of revolutionaries as one can imagine,” Pulitzer Prize–winning historian Jack Rakove recounts. “They became revolutionaries despite themselves.” In the years leading up to the war, John Adams feared British retaliation and hesitated to give up his budding law career; he only got involved after being elected as a delegate to the First Continental Congress. George Washington had been focused on managing his wheat, flour, fishing, and horse-breeding businesses, joining the cause only after Adams nominated him as commander in chief of the army. “I have used every endeavor in my power to avoid it,” Washington wrote.
Nearly two centuries later, Martin Luther King, Jr., was apprehensive about leading the civil rights movement; his dream was to be a pastor and a college president. In 1955, after Rosa Parks was tried for refusing to give up her seat at the front of a bus, a group of civil rights activists gathered to discuss their response. They agreed to form the Montgomery Improvement Association and launch a bus boycott, and one of the attendees nominated King for the presidency. “It had happened so quickly that I did not even have time to think it through. It is probable that if I had, I would have declined the nomination,” King reflected. Just three weeks earlier, King and his wife had “agreed that I should not then take on any heavy community responsibilities, since I had so recently finished my thesis, and needed to give more attention to my church work.” He was unanimously elected to lead the boycott. Faced with giving a speech to the community that evening, “I became possessed by fear.” King would overcome that trepidation soon enough that in 1963 his thundering voice united a country around an electrifying vision of freedom. But that only happened because a colleague proposed that King should be the closing speaker at the March on Washington and gathered a coalition of leaders to advocate for him.
When the pope commissioned him to paint a fresco on the ceiling of the Sistine Chapel, Michelangelo wasn’t interested. He viewed himself as a sculptor, not a painter, and found the task so overwhelming that he fled to Florence. Two years would pass before he began work on the project, at the pope’s insistence. And astronomy stagnated for decades because Nicolaus Copernicus refused to publish his original discovery that the earth revolves around the sun. Fearing rejection and ridicule, he stayed silent for twenty-two years, circulating his findings only to his friends. Eventually, a major cardinal learned of his work and wrote a letter encouraging Copernicus to publish it. Even then, Copernicus stalled for four more years. His magnum opus only saw the light of day after a young mathematics professor took matters into his own hands and submitted it for publication.
Almost half a millennium later, when an angel investor offered $250,000 to Steve Jobs and Steve Wozniak to bankroll Apple in 1977, it came with an ultimatum: Wozniak would have to leave Hewlett-Packard. He refused. “I still intended to be at that company forever,” Wozniak reflects. “My psychological block was really that I didn’t want to start a company. Because I was just afraid,” he admits. Wozniak changed his mind only after being encouraged by Jobs, multiple friends, and his own parents.
We can only imagine how many Wozniaks, Michelangelos, and Kings never pursued, publicized, or promoted their original ideas because they were not dragged or catapulted into the spotlight. Although we may not all aspire to start our own companies, create a masterpiece, transform Western thought, or lead a civil rights movement, we do have ideas for improving our workplaces, schools, and communities. Sadly, many of us hesitate to take action to promote those ideas. As economist Joseph Schumpeter famously observed, originality is an act of creative destruction. Advocating for new systems often requires demolishing the old way of doing things, and we hold back for fear of rocking the boat. Among nearly a thousand scientists at the Food and Drug Administration, more than 40 percent were afraid that they would face retaliation if they spoke up publicly about safety concerns. Of more than forty thousand employees at a technology company, half felt it was not safe to voice dissenting opinions at work. When employees in consulting, financial services, media, pharmaceuticals, and advertising companies were interviewed, 85 percent admitted to keeping quiet about an important concern rather than voicing it to their bosses.
The last time you had an original idea, what did you do with it? Although America is a land of individuality and unique self-expression, in search of excellence and in fear of failure, most of us opt to fit in rather than stand out. “On matters of style, swim with the current,” Thomas Jefferson allegedly advised, but “on matters of principle, stand like a rock.” The pressure to achieve leads us to do the opposite. We find surface ways of appearing original—donning a bow tie, wearing bright red shoes—without taking the risk of actually being original. When it comes to the powerful ideas in our heads and the core values in our hearts, we censor ourselves. “There are so few originals in life,” says renowned executive Mellody Hobson, because people are afraid to “speak up and stand out.” What are the habits of the people whose originality extends beyond appearance to effective action?
The Right Stuff
To be an original, you need to take radical risks. This belief is embedded so deeply in our cultural psyche that we rarely even stop to think about it. We admire astronauts like Neil Armstrong and Sally Ride for having “the right stuff”—the courage to leave the only planet humans have ever inhabited and venture boldly into space. We celebrate heroes like Mahatma Gandhi and Martin Luther King, Jr., who possessed enough conviction to risk their lives for the moral principles they held dear. We idolize icons like Steve Jobs and Bill Gates for having the audacity to drop out of school and go for broke, holing up in garages to will their technological visions into existence.
When we marvel at the original individuals who fuel creativity and drive change in the world, we tend to assume they’re cut from a different cloth. In the same way that some lucky people are born with genetic mutations that make them resistant to diseases like cancer, obesity, and HIV, we believe that great creators are born with a biological immunity to risk. They’re wired to embrace uncertainty and ignore social approval; they simply don’t worry about the costs of non-conformity the way the rest of us do. They’re programmed to be iconoclasts, rebels, revolutionaries, troublemakers, mavericks, and contrarians who are impervious to fear, rejection, and ridicule.
The word entrepreneur, as it was coined by economist Richard Cantillon, literally means “bearer of risk.” When we read the story of Warby Parker’s stratospheric rise, this theme comes through loud and clear. Like all great creators, innovators, and change agents, the quartet transformed the world because they were willing to take a leap of faith. After all, if you don’t swing for the fences, it’s impossible to hit a home run.
Isn’t it?
—
Six months before Warby Parker launched, one of the founders was sitting in my classroom at Wharton. Tall and affable, with curly black hair and a calm energy, Neil Blumenthal hailed from a nonprofit background and genuinely aspired to make the world a better place. When he pitched the company to me, like many other doubters, I told him it sounded like an interesting idea, but it was hard to imagine people ordering glasses online.
With a skeptical consumer base, I knew, it would require a herculean effort to get the company off the ground. And when I learned how Neil and his friends were spending their time preparing for the launch, I had the sinking feeling that they were doomed.
The first strike against them, I told Neil, was that they were all still in school. If they truly believed in Warby Parker, they should drop out to focus every waking hour on making it happen.
“We want to hedge our bets,” he responded. “We’re not sure if it’s a good idea and we have no clue whether it will succeed, so we’ve been working on it in our spare time during the school year. We were four friends before we started, and we made a commitment that dealing with each other fairly was more important than success. But for the summer, Jeff got a grant to focus on the business full time.”
What about the other three of you? “We all got internships,” Neil admitted. “I was in consulting, Andy was in venture capital, and Dave was in health care.”
With their time scarce and their attention divided, they still hadn’t built a website, and it had taken them six months just to agree on a name for the company. Strike two.
Before I gave up on them entirely, though, I remembered that they were all graduating at the end of the year, which meant they’d finally have the time to go all in and dedicate themselves completely to the business. “Well, not necessarily,” Neil backpedaled. “We’ve hedged our bets. Just in case things don’t work out, I’ve accepted a full-time job for after graduation. So has Jeff. And to make sure he would have options, Dave did two different internships over the summer, and he’s talking with his former employer about rejoining.”
Strike three. They were out—and so was I.
I declined to invest in Warby Parker because Neil and his friends were too much like me. I became a professor because I was passionate about discovering new insights, sharing knowledge, and teaching the next generations of students. But in my most honest moments, I know that I was also drawn to the security of tenure. I would never have had the confidence to start a business in my twenties. If I had, I certainly would have stayed in school and lined up a job to cover my bases.
When I compared the choices of the Warby Parker team to my mental model of the choices of successful entrepreneurs, they didn’t match. Neil and his colleagues lacked the guts to go in with their guns blazing, which led me to question their conviction and commitment. They weren’t serious about becoming successful entrepreneurs: They didn’t have enough skin in the game. In my mind, they were destined to fail because they played it safe instead of betting the farm. But in fact, this is exactly why they succeeded.
I want to debunk the myth that originality requires extreme risk taking and persuade you that originals are actually far more ordinary than we realize. In every domain, from business and politics to science and art, the people who move the world forward with original ideas are rarely paragons of conviction and commitment. As they question traditions and challenge the status quo, they may appear bold and self-assured on the surface. But when you peel back the layers, the truth is that they, too, grapple with fear, ambivalence, and self-doubt. We view them as self-starters, but their efforts are often fueled and sometimes forced by others. And as much as they seem to crave risk, they really prefer to avoid it.
—
In a fascinating study, management researchers Joseph Raffiee and Jie Feng asked a simple question: When people start a business, are they better off keeping or quitting their day jobs? From 1994 until 2008, they tracked a nationally representative group of over five thousand Americans in their twenties, thirties, forties, and fifties who became entrepreneurs. Whether these founders kept or left their day jobs wasn’t influenced by financial need; individuals with high family income or high salaries weren’t any more or less likely to quit and become full-time entrepreneurs. A survey showed that the ones who took the full plunge were risk takers with spades of confidence. The entrepreneurs who hedged their bets by starting their companies while still working were far more risk averse and unsure of themselves.
If you think like most people, you’ll predict a clear advantage for the risk takers. Yet the study showed the exact opposite: Entrepreneurs who kept their day jobs had 33 percent lower odds of failure than those who quit.
If you’re risk averse and have some doubts about the feasibility of your ideas, it’s likely that your business will be built to last. If you’re a freewheeling gambler, your startup is far more fragile.
Like the Warby Parker crew, the entrepreneurs whose companies topped Fast Company’s recent most innovative lists typically stayed in their day jobs even after they launched. Former track star Phil Knight started selling running shoes out of the trunk of his car in 1964, yet kept working as an accountant until 1969. After inventing the original Apple I computer, Steve Wozniak started the company with Steve Jobs in 1976 but continued working full time in his engineering job at Hewlett-Packard until 1977. And although Google founders Larry Page and Sergey Brin figured out how to dramatically improve internet searches in 1996, they didn’t go on leave from their graduate studies at Stanford until 1998. “We almost didn’t start Google,” Page says, because we “were too worried about dropping out of our Ph.D. program.” In 1997, concerned that their fledgling search engine was distracting them from their research, they tried to sell Google for less than $2 million in cash and stock. Luckily for them, the potential buyer rejected the offer.
This habit of keeping one’s day job isn’t limited to successful entrepreneurs. Many influential creative minds have stayed in full-time employment or education even after earning income from major projects. Selma director Ava DuVernay made her first three films while working in her day job as a publicist, only pursuing filmmaking full time after working at it for four years and winning multiple awards. Brian May was in the middle of doctoral studies in astrophysics when he started playing guitar in a new band, but he didn’t drop out until several years later to go all in with Queen. Soon thereafter he wrote “We Will Rock You.” Grammy winner John Legend released his first album in 2000 but kept working as a management consultant until 2002, preparing PowerPoint presentations by day while performing at night. Thriller master Stephen King worked as a teacher, janitor, and gas station attendant for seven years after writing his first story, only quitting a year after his first novel, Carrie, was published. Dilbert author Scott Adams worked at Pacific Bell for seven years after his first comic strip hit newspapers.
Why did all these originals play it safe instead of risking it all?
Why Risks Are Like Stock Portfolios
Half a century ago, University of Michigan psychologist Clyde Coombs developed an innovative theory of risk. In the stock market, if you’re going to make a risky investment, you protect yourself by playing it safe in other investments. Coombs suggested that in their daily lives, successful people do the same thing with risks, balancing them out in a portfolio. When we embrace danger in one domain, we offset our overall level of risk by exercising caution in another domain. If you’re about to bet aggressively in blackjack, you might drive below the speed limit on your way to the casino.
Risk portfolios explain why people often become original in one part of their lives while remaining quite conventional in others. Baseball owner Branch Rickey opened the door for Jackie Robinson to break the color barrier, but refused to go to the ballpark on Sundays, use profanity, or touch a drop of alcohol. T. S. Eliot’s landmark work, The Waste Land, has been hailed as one of the twentieth century’s most significant poems. But after publishing it in 1922, Eliot kept his London bank job until 1925, rejecting the idea of embracing professional risk. As the novelist Aldous Huxley noted after paying him an office visit, Eliot was “the most bank-clerky of all bank clerks.” When he finally did leave the position, Eliot still didn’t strike out on his own. He spent the next forty years working for a publishing house to provide stability in his life, writing poetry on the side. As Polaroid founder Edwin Land remarked, “No person could possibly be original in one area unless he were possessed of the emotional and social stability that comes from fixed attitudes in all areas other than the one in which he is being original.”
But don’t day jobs distract us from doing our best work? Common sense suggests that creative accomplishments can’t flourish without big windows of time and energy, and companies can’t thrive without intensive effort. Those assumptions overlook the central benefit of a balanced risk portfolio: Having a sense of security in one realm gives us the freedom to be original in another. By covering our bases financially, we escape the pressure to publish half-baked books, sell shoddy art, or launch untested businesses. When Pierre Omidyar built eBay, it was just a hobby; he kept working as a programmer for the next nine months, only leaving after his online marketplace was netting him more money than his job. “The best entrepreneurs are not risk maximizers,” Endeavor cofounder and CEO Linda Rottenberg observes based on decades of experience training many of the world’s great entrepreneurs. “They take the risk out of risk-taking.”
Managing a balanced risk portfolio doesn’t mean constantly hovering in the middle of the spectrum by taking moderate risks. Instead, successful originals take extreme risks in one arena and offset them with extreme caution in another. At age twenty-seven, Sara Blakely generated the novel idea of creating footless pantyhose, taking a big risk by investing her entire savings of $5,000. To balance out her risk portfolio, she stayed in her full-time position selling fax machines for two years, spending nights and weekends building the prototype—and saving money by writing her own patent application instead of hiring lawyers to do so. After she finally launched Spanx, she became the world’s youngest self-made billionaire. A century earlier, Henry Ford started his automotive empire while employed as a chief engineer for Thomas Edison, which gave him the security necessary to try out his novel inventions for a car. He continued working under Edison for two years after building a carburetor and a year after earning a patent for it.
And what about Bill Gates, famous for dropping out of Harvard to start Microsoft? When Gates sold a new software program as a sophomore, he waited an entire year before leaving school. Even then he didn’t drop out, but balanced his risk portfolio by applying for a leave of absence that was formally approved by the university—and by having his parents bankroll him. “Far from being one of the world’s great risk takers,” entrepreneur Rick Smith notes, “Bill Gates might more accurately be thought of as one of the world’s great risk mitigators.”
It was this kind of risk mitigation that was responsible for Warby Parker’s breakthrough. Two of the cofounders, Neil Blumenthal and Dave Gilboa, became the company’s co-CEOs. They rejected advice to conform to the norm of selecting a single leader, believing it was safer to have a pair at the helm—indeed, evidence shows that having co-CEOs elicits positive market reactions and increases firm valuation. From the start, their number-one priority was reducing risk. “Warby Parker wasn’t the basket that I wanted to put all my eggs into,” Dave says. After starting the company he continued exploring other business opportunities by scouting scientific discoveries on campus to see if they had any commercial potential. Having backup plans gave the founders the courage to base their business on the unproven assumption that people would be willing to buy glasses online. Instead of just acknowledging that uncertainty, they actively worked to minimize it. “We talked constantly about de-risking the business,” Neil says. “The whole journey was a series of go/no-go decisions. At every step of the way, we had checks and balances.”
As part of their protection against risk, the four friends took an entrepreneurship class together and spent months honing their business plan. To make customers more comfortable with the unfamiliar concept of ordering eyewear over the internet, they decided to offer free returns. But in surveys and focus groups, people were still hesitant to buy glasses online. “There were a lot of people who just wouldn’t do it. That really made us question the whole premise of the business,” Neil recalls. “It was a moment of severe self-doubt. That took us back to the drawing board.”
After discussing the problem at length, the team came up with a solution—a free home try-on program. Customers could order the frames alone without any financial commitment, and simply send them back if they didn’t like the feel or look. This would actually be less costly than free returns. If a customer bought the frames with lenses and then returned them, Warby Parker would lose a lot of money, as the lenses were unique to the customer. But if customers tried on only the frames and returned them, the company could reuse them. By now Dave was confident and committed: “By the time we were ready to launch, and I had to make the decision this was something we were ready to do full time, it didn’t seem risky. It didn’t feel like I was taking a big leap of faith.” The free home try-on program was so popular that Warby Parker had to temporarily suspend it within forty-eight hours of launch.
A growing body of evidence suggests that entrepreneurs don’t like risk any more than the rest of us—and it’s the rare conclusion on which many economists, sociologists, and psychologists have actually come to agree. In one representative study of over eight hundred Americans, entrepreneurs and employed adults were asked to choose which of the following three ventures they would prefer to start:
	(a) One that made $5 million in profit with a 20 percent chance of success
	(b) One that made $2 million in profit with a 50 percent chance of success
	(c) One that made $1.25 million in profit with an 80 percent chance of success
The entrepreneurs were significantly more likely to choose the last option, the safest one. This was true regardless of income, wealth, age, gender, entrepreneurial experience, marital status, education, household size, and expectations of how well other businesses would perform. “We find that entrepreneurs are significantly more risk-averse than the general population,” the authors conclude.
These are just preferences on a survey, but when you track entrepreneurs’ real-world behavior, it’s clear that they avoid dangerous risks. Economists find that as teenagers, successful entrepreneurs were nearly three times as likely as their peers to break rules and engage in illicit activities. Yet when you take a closer look at the specific behaviors involved, the adolescents who went on to start productive companies were only taking calculated risks. When psychologists studied American twins and Swedish citizens, they found the same results.
Across all three studies, the people who become successful entrepreneurs were more likely to have teenage histories of defying their parents, staying out past their curfews, skipping school, shoplifting, gambling, drinking alcohol, and smoking marijuana. They were not, however, more likely to engage in hazardous activities like driving drunk, buying illegal drugs, or stealing valuables. And that was true regardless of their parents’ socioeconomic status or family income.
Originals do vary in their attitudes toward risk. Some are skydiving gamblers; others are penny-pinching germophobes. To become original, you have to try something new, which means accepting some measure of risk. But the most successful originals are not the daredevils who leap before they look. They are the ones who reluctantly tiptoe to the edge of a cliff, calculate the rate of descent, triple-check their parachutes, and set up a safety net at the bottom just in case. As Malcolm Gladwell wrote in the New Yorker, “Many entrepreneurs take plenty of risks—but those are generally the failed entrepreneurs, not the success stories.”
A disregard for social approval doesn’t differentiate people who take original paths, either. In a comprehensive analysis of 60 studies covering more than 15,000 entrepreneurs, people who had little concern for pleasing others weren’t more likely to become entrepreneurs, nor did their firms perform any better. We see the same pattern in politics: When hundreds of historians, psychologists, and political scientists evaluated America’s presidents, they determined that the least effective leaders were those who followed the will of the people and the precedents set by their predecessors. The greatest presidents were those who challenged the status quo and brought about sweeping changes that improved the lot of the country. But these behaviors were completely unrelated to whether they cared deeply about public approval and social harmony.
Abraham Lincoln is usually regarded as the greatest of all American presidents. When experts rated the presidents on the desire to please others and avoid conflict, Lincoln scored the highest of them all. He devoted four hours a day to holding office hours with citizens and pardoned deserters during the Civil War. Before signing the Emancipation Proclamation, Lincoln agonized for six months over whether he should free the slaves. He questioned whether he had the constitutional authority; he worried that the decision might lose him the support of the border states, forfeit the war, and destroy the country.
Originality is not a fixed trait. It is a free choice. Lincoln wasn’t born with an original personality. Taking on controversy wasn’t programmed into his DNA; it was an act of conscious will. As the great thinker W. E. B. DuBois wrote, “He was one of you and yet he became Abraham Lincoln.”
Too often that possibility of control is missing from our work and our lives. A few years ago, Google asked a brilliant Yale professor named Amy Wrzesniewski to help enrich the jobs of employees in sales and administrative positions, who didn’t have the same perceived freedom, status, or moon-shot projects as the company’s engineers. I joined her and another collaborator, Justin Berg, on a trip to California, New York, Dublin, and London in search of a solution.
Many employees were so committed to Google that they accepted their own jobs as defaults. Since they saw their tasks and interactions as set like plaster, they did not question whether they could adjust them.
To unlock their mindsets, we partnered with Jennifer Kurkoski and Brian Welle, two innovators behind Google’s people analytics work. We designed a workshop introducing hundreds of employees to the notion that jobs are not static sculptures, but flexible building blocks. We gave them examples of people becoming the architects of their own jobs, customizing their tasks and relationships to better align with their interests, skills, and values—like an artistic salesperson volunteering to design a new logo and an outgoing financial analyst communicating with clients using video chat instead of email. Then, they looked at their familiar jobs in an unfamiliar way: vuja de. They set out to create a new vision of their roles that was more ideal but still realistic.
Managers and coworkers rated each employee’s happiness and performance before the workshop and after several weeks and months had passed. The whole experience lasted only ninety minutes, so we weren’t sure that it would be enough to make a difference. But six weeks later, Googlers who were randomly assigned to think about their jobs as malleable showed a spike in happiness and performance. Having considered how their jobs could be modified, they had taken action to improve them. Employees in a control group who didn’t attend the same workshop didn’t show any changes in happiness or performance. When we added a feature to encourage employees to see both their skills and jobs as flexible, the gains lasted for at least six months. Instead of using only their existing talents, they took the initiative to develop new capabilities that enabled them to create an original, personalized job. As a result, they were 70 percent more likely than their peers to land a promotion or a transition to a coveted role. By refusing to stick with their default jobs and default skills, they became happier and more effective—and qualified themselves for roles that were a better fit. Many of their limits, they came to realize, were of their own making.
—
Having revealed that successful originals often begin by questioning defaults and balancing risk portfolios, the rest of this book is about closing the gap between insight and action. Once you have a new idea, how do you champion it effectively? As an organizational psychologist at Wharton, I’ve spent more than a decade studying originality in a wide range of settings, from technology companies and banks to schools, hospitals, and governments. I’ve also sought out some of the most prominent originals of our time, and I want to share their wisdom about how we can all be more original without jeopardizing our relationships, reputations, and careers. I hope my findings will help people develop the courage and strategies to pursue originality, and give leaders the knowledge necessary to create cultures of originality in their teams and organizations.
Using studies and stories spanning business, politics, sports, and entertainment, I’ll look at the seeds of creative, moral, and organizational change—and the barriers that hinder progress. The first section of this book focuses on managing the risks involved in generating, recognizing, and voicing original ideas. By definition, new ideas are fraught with uncertainty, and powerful evidence illuminates how we can hone our skills in separating the wheat from the chaff, to avoid the risks of betting on bad ideas and passing on good ones. After you spot a promising idea, the next step is to communicate it effectively. I’ll share some best practices for speaking up, shedding light on how to select the messages and audiences to get heard more and punished less. Along the way, you’ll find out why the most popular television show of all time narrowly escaped the cutting-room floor, why an entrepreneur pitches his startups by highlighting the reasons not to invest in them, how a CIA analyst convinced the intelligence community to stop being so secretive, and how a woman at Apple challenged Steve Jobs from three levels below—and won.
The second section of the book deals with the choices that we make to scale originality. I’ll start with the dilemma of timing: It turns out that you should be wary of being the first mover, because it’s often riskier to act early than late. Unexpectedly, some of the greatest creative achievements and change initiatives in history have their roots in procrastination, and the tendency to delay and postpone can help entrepreneurs build companies that last, leaders guide transformation efforts, and innovators maintain their originality. I’ll then turn to the challenges of coalition building, investigating how to grow support for an original idea and reduce the risks of rejection. The unsung hero of the women’s suffrage movement will illustrate why enemies make better allies than frenemies, and shared values can divide rather than unite. A founder who hid her firm’s mission from employees and a Hollywood director who shifted Disney’s direction for animated films will demonstrate how to recruit collaborators by balancing idealism with pragmatism and blending the familiar with the new.
The third section of the book concerns unleashing and sustaining originality, both at home and in work. I’ll examine how to nurture originality in children, explaining how parents, siblings, and role models shape our tendencies to rebel. You’ll see why the number of bases that professional baseball players steal can be traced to their birth order, the most original comedians in America come from similar family backgrounds, the people who risked their lives to perform heroic rescues during the Holocaust received a particular kind of discipline from their parents, and the innovation and economic growth rates of entire countries can be traced to the books we read to our children. From there, I’ll consider why some cultures become cults, and how leaders can encourage dissenting opinions that allow originality to flourish. You’ll learn from a billionaire financial wizard who fires employees for failing to criticize him, an inventor who struggled to spread his ingenuity, and an expert who helped change the norm of silence at NASA after the space shuttle Columbia exploded.
I’ll close with reflections on the emotions that hold us back from pursuing originality. You’ll gain insight on overcoming fear and apathy from a group of twentysomethings who toppled a tyrant and a lawyer who fought climate change by swimming the North Pole. Their examples underscore evidence that calming down isn’t the best way to manage anxiety, that venting backfires when we’re angry, and that pessimism is sometimes more energizing than optimism.
Ultimately, the people who choose to champion originality are the ones who propel us forward. After spending years studying them and interacting with them, I am struck that their inner experiences are not any different from our own. They feel the same fear, the same doubt, as the rest of us. What sets them apart is that they take action anyway. They know in their hearts that failing would yield less regret than failing to try.
2
Blind Inventors and One-Eyed Investors
The Art and Science of Recognizing Original Ideas
	“Creativity is allowing yourself to make mistakes. 
	Art is knowing which ones to keep.”
	Scott Adams
	A
t the turn of the century, an invention took Silicon Valley by storm. Steve Jobs called it the most amazing piece of technology since the personal computer. Enamored with the prototype, Jobs offered the inventor $63 million for 10 percent of the company. When the inventor turned it down, Jobs did something out of character: he offered to advise the inventor for the next six months—for free. Amazon founder Jeff Bezos took one look at the product and immediately got involved, telling the inventor, “You have a product so revolutionary, you’ll have no problem selling it.” John Doerr, the legendary investor who bet successfully on Google and many other blue-chip startups, pumped $80 million into the business, predicting that it would be the fastest company ever to reach $1 billion and “would become more important than the internet.”
The inventor himself was described as a modern Thomas Edison—he already had a track record of remarkable breakthroughs. His portable dialysis machine had been named the medical product of the year, his portable drug infusion pump reduced the time that patients were stuck in hospitals, and his vascular stent is connected to Vice President Dick Cheney’s heart. He had accumulated hundreds of patents and received America’s highest honor for invention, the National Medal of Technology and Innovation, from President Bill Clinton.
The inventor projected that within a year, sales of his newest product would reach 10,000 units a week. But six years later, they had sold only about 30,000 units in total. After more than a decade, the company still hadn’t become profitable. It was supposed to transform lives and cities, but today it is used only in niche markets.
That product was the Segway, the self-balancing personal transporter. Time called it one of the ten biggest technology flops of the decade. “Segway as an investment was a failure, no question about it,” Doerr admitted in 2013. “I made some pretty bold predictions about Segway that were wrong.” Why did such savvy business minds all miss the mark?
Some years earlier, two entertainers got together to create a 90-minute television special. They had no experience writing for the medium and quickly ran out of material, so they shifted their concept to a half-hour weekly show. When they submitted their script, most of the network executives didn’t like it or didn’t get it. One of the actors involved in the program described it as a “glorious mess.”
After filming the pilot, it was time for an audience test. The one hundred viewers who were assembled in Los Angeles to discuss the strengths and weaknesses of the show dismissed it as a dismal failure. One put it bluntly: “He’s just a loser, who’d want to watch this guy?” After about six hundred additional people were shown the pilot in four different cities, the summary report concluded: “No segment of the audience was eager to watch the show again.” The performance was rated weak.
The pilot episode squeaked onto the airwaves, and as expected, it wasn’t a hit. Between that and the negative audience tests, the show should have been toast. But one executive campaigned to have four more episodes made. They didn’t go live until nearly a year after the pilot, and again, they failed to gain a devoted following. With the clock winding down, the network ordered half a season as replacement for a canceled show, but by then one of the writers was ready to walk away: he didn’t have any more ideas.
It’s a good thing he changed his mind. Over the next decade, the show dominated the Nielsen ratings and brought in over $1 billion in revenues. It became the most popular TV series in America, and TV Guide named it the greatest program of all time.
If you’ve ever complained about a close talker, accused a partygoer of double-dipping a chip, uttered the disclaimer “Not that there’s anything wrong with that,” or rejected someone by saying “No soup for you,” you’re using phrases coined on the show. Why did network executives have so little faith in Seinfeld?
When we bemoan the lack of originality in the world, we blame it on the absence of creativity. If only people could generate more novel ideas, we’d all be better off. But in reality, the biggest barrier to originality is not idea generation—it’s idea selection. In one analysis, when over two hundred people dreamed up more than a thousand ideas for new ventures and products, 87 percent were completely unique. Our companies, communities, and countries don’t necessarily suffer from a shortage of novel ideas. They’re constrained by a shortage of people who excel at choosing the right novel ideas. The Segway was a false positive: it was forecast as a hit but turned out to be a miss. Seinfeld was a false negative: it was expected to fail but ultimately flourished.
This chapter is about the hurdles and best practices in idea selection. To figure out how we can make fewer bad bets, I sought out skilled forecasters who have learned to avoid the risks of false positives and false negatives. You’ll meet two venture capitalists who anticipated the failure of the Segway, and the NBC executive who didn’t even work in comedy but was so enthusiastic about the Seinfeld pilot that he went out on a limb to fund it. Their methods question conventional wisdom about the relative importance of intuition and analysis in assessing ideas, and about how we should weigh passion in evaluating the people behind those ideas. You’ll see why it’s so difficult for managers and test audiences to accurately evaluate new ideas, and how we can get better at deciding when to roll the dice.
A Random Walk on the Creative Tightrope
The inventor of the Segway is a technological whiz named Dean Kamen, whose closet is stocked with one outfit: a denim shirt, jeans, and work boots. When I asked venture capitalists to describe Kamen, the most common response was “Batman.” At sixteen, he took it upon himself to redesign a museum’s lighting system—and only then sought the chairman’s permission to implement it. In the 1970s, he invented the drug infusion pump, which was profitable enough that he bought a jet and a helicopter and built a mansion in New Hampshire, complete with a machine shop, an electronics lab, and a baseball field. In the 1980s, his portable dialysis machine was a massive success.
In the 1990s, Kamen designed the iBOT, a wheelchair that could climb stairs. Recognizing broader applications for the technology, he brought in a team to help create the Segway. The goal was to build a safe, fuel-efficient vehicle that would prevent pollution and help individuals navigate congested cities. Because it was small, lightweight, and balanced on its own, it would be a natural fit for mail carriers, police officers, and golfers, but also had the potential to fundamentally alter everyday transportation. The Segway was the most extraordinary technology he had ever created, and Kamen predicted that it would “be to the car what the car was to the horse and buggy.”
But can creators ever be objective in judging their own ideas? One of my former students, Justin Berg, is now a wunderkind professor at Stanford who has spent years investigating this question. Berg specializes in creative forecasting, the art of predicting the success of novel ideas. In one study, he showed different groups of people videos of circus performances and asked them to make projections about how well each would do. Circus artists from Cirque du Soleil and other organizations submitted predictions about how popular their videos would be. Circus managers watched the videos and registered their predictions, too.
To test the accuracy of their forecasts, Berg then measured the actual success of each performance by tracking how much general audience members liked, shared, and funded the videos. He invited over thirteen thousand people to rate the videos; they also had a chance to share them via Facebook, Twitter, Google+, and email, and received a ten-cent bonus that they could donate to the performers.
The creators proved to be terrible at judging how their performances would do with the test audiences. On average, when ranking their videos against the performances of nine other circus artists, they put their own work two slots too high. The managers were more realistic: they had some distance from the performances, which put them in a more neutral position.
Social scientists have long known that we tend to be overconfident when we evaluate ourselves. Here are some highlights of their findings:
• High school seniors: 70 percent report that they have “above average” leadership skills, compared with 2 percent “below average”; in the ability to get along with others, 25 percent rate themselves in the top 1 percent, and 60 percent put themselves in the top 10 percent.
• College professors: 94 percent rate themselves as doing above-average work.
• Engineers: In two different companies, 32 percent and 42 percent rated themselves among the top 5 percent of performers.
• Entrepreneurs: When 3,000 small-business owners rated the probability that different companies would succeed, on average they rated the prospects of their own businesses as 8.1 out of 10 but gave similar enterprises odds of only 5.9 out of 10.
—
Overconfidence may be a particularly difficult bias to overcome in the creative domain. When you’re generating a new idea, by definition it’s unique, so you can ignore all the feedback you’ve received in the past about earlier inventions. Even if your previous ideas have bombed, this one is different.
When we’ve developed an idea, we’re typically too close to our own tastes—and too far from the audience’s taste—to evaluate it accurately. We’re giddy from the thrill of the eureka moment or the triumph of overcoming an obstacle. As Brandon Tartikoff, NBC’s longtime entertainment president, frequently reminded his producers, “Nobody walks in here with what they think is a bad idea.” To some degree, entrepreneurs and inventors have to be overconfident about the odds of their ideas succeeding, or they wouldn’t have the motivational fuel to pursue them. But even when they do learn about their audience’s preferences, it’s too easy for them to fall victim to what psychologists call confirmation bias: they focus on the strengths of their ideas while ignoring or discounting their limitations.
After spending his career studying creative productivity, psychologist Dean Simonton has found that even geniuses have trouble recognizing when they have a hit on their hands. In music, Beethoven was known as a perceptive self-critic, yet as Simonton observes, “Beethoven’s own favorites among his symphonies, sonatas, and quartets are not those most frequently performed and recorded by posterity.” In one analysis, psychologist Aaron Kozbelt pored over letters in which Beethoven evaluated seventy of his compositions, comparing those appraisals to expert judgments of Beethoven’s works. In that set of seventy, Beethoven committed fifteen false positives, expecting pieces to be major that turned out to be minor, and only eight false negatives, criticizing pieces that ended up becoming highly valued. This 33 percent error rate occurred despite the fact that Beethoven made many of his assessments after receiving audience feedback.
If creators knew when they were on their way to fashioning a masterpiece, their work would progress only forward: they would halt their idea-generation efforts as they struck gold. But time and again, Simonton finds that they backtrack, returning to iterations that they had earlier discarded as inadequate. In Beethoven’s most celebrated work, the Fifth Symphony, he scrapped the conclusion of the first movement because it felt too short, only to come back to it later. Had Beethoven been able to distinguish an extraordinary from an ordinary work, he would have accepted his composition immediately as a hit. When Picasso was painting his famous Guernica in protest of fascism, he produced seventy-nine different drawings. Many of the images in the painting were based on his early sketches, not the later variations. “The subsequent sketches proved to constitute ‘blind alleys’ in which the artist did not know in advance that he was taking the wrong track,” Simonton explains. If Picasso could judge his creations as he produced them, he would get consistently “warmer” and use the later drawings. But in reality, it was just as common that he got “colder.”
Kissing Frogs
If originals aren’t reliable judges of the quality of their ideas, how do they maximize their odds of creating a masterpiece? They come up with a large number of ideas. Simonton finds that on average, creative geniuses weren’t qualitatively better in their fields than their peers. They simply produced a greater volume of work, which gave them more variation and a higher chance of originality. “The odds of producing an influential or successful idea,” Simonton notes, are “a positive function of the total number of ideas generated.”
Consider Shakespeare: we’re most familiar with a small number of his classics, forgetting that in the span of two decades, he produced 37 plays and 154 sonnets. Simonton tracked the popularity of Shakespeare’s plays, measuring how often they’re performed and how widely they’re praised by experts and critics. In the same five-year window that Shakespeare produced three of his five most popular works—Macbeth, King Lear, and Othello—he also churned out the comparatively average Timon of Athens and All’s Well That Ends Well, both of which rank among the worst of his plays and have been consistently slammed for unpolished prose and incomplete plot and character development.
In every field, even the most eminent creators typically produce a large quantity of work that’s technically sound but considered unremarkable by experts and audiences. When the London Philharmonic Orchestra chose the 50 greatest pieces of classical music, the list included six pieces by Mozart, five by Beethoven, and three by Bach. To generate a handful of masterworks, Mozart composed more than 600 pieces before his death at thirty-five, Beethoven produced 650 in his lifetime, and Bach wrote over a thousand. In a study of over 15,000 classical music compositions, the more pieces a composer produced in a given five-year window, the greater the spike in the odds of a hit.
Picasso’s oeuvre includes more than 1,800 paintings, 1,200 sculptures, 2,800 ceramics, and 12,000 drawings, not to mention prints, rugs, and tapestries—only a fraction of which have garnered acclaim. In poetry, when we recite Maya Angelou’s classic poem “Still I Rise,” we tend to forget that she wrote 165 others; we remember her moving memoir I Know Why the Caged Bird Sings and pay less attention to her other 6 autobiographies. In science, Einstein wrote papers on general and special relativity that transformed physics, but many of his 248 publications had minimal impact. If you want to be original, “the most important possible thing you could do,” says Ira Glass, the producer of This American Life and the podcast Serial, “is do a lot of work. Do a huge volume of work.”
Across fields, Simonton reports that the most prolific people not only have the highest originality; they also generate their most original output during the periods in which they produce the largest volume.* Between the ages of thirty and thirty-five, Edison pioneered the lightbulb, the phonograph, and the carbon telephone. But during that period, he filed well over one hundred patents for other inventions as diverse as stencil pens, a fruit preservation technique, and a way of using magnets to mine iron ore—and designed a creepy talking doll. “Those periods in which the most minor products appear tend to be the same periods in which the most major works appear,” Simonton notes. Edison’s “1,093 patents notwithstanding, the number of truly superlative creative achievements can probably be counted on the fingers of one hand.”
It’s widely assumed that there’s a tradeoff between quantity and quality—if you want to do better work, you have to do less of it—but this turns out to be false. In fact, when it comes to idea generation, quantity is the most predictable path to quality. “Original thinkers,” Stanford professor Robert Sutton notes, “will come up with many ideas that are strange mutations, dead ends, and utter failures. The cost is worthwhile because they also generate a larger pool of ideas—especially novel ideas.”
Many people fail to achieve originality because they generate a few ideas and then obsess about refining them to perfection. At Upworthy, the company that makes good content go viral, two different staff members wrote headlines for a video of monkeys reacting to receiving cucumbers or grapes as rewards. Eight thousand people watched it when the headline was “Remember Planet of the Apes? It’s Closer to Reality than You Think.” A different headline led to fifty-nine times more views, enticing nearly half a million people to watch the same video: “2 Monkeys Were Paid Unequally; See What Happens Next.” Upworthy’s rule is that you need to generate at least twenty-five headline ideas to strike gold. Backtracking studies show that wizards do sometimes come up with novel ideas early in the creative process. But for the rest of us, our first ideas are often the most conventional—the closest to the default that already exists. It’s only after we’ve ruled out the obvious that we have the greatest freedom to consider the more remote possibilities. “Once you start getting desperate, you start thinking outside the box,” the Upworthy team writes. “#24 will suck. Then #25 will be a gift from the headline gods and will make you a legend.”
While working on the Segway, Dean Kamen was aware of the blind variations that mark the creative process. With more than 440 patents to his name, he had plenty of misses as well as hits. “You gotta kiss a lot of frogs,” he often told his team, “before you find a prince.” In fact, frog kissing was one of his mantras: he encouraged his engineers to try out many variations to increase their chances of stumbling on the right one. But he settled on the Segway before considering other ideas for solving transportation problems, losing sight of the fact that inventors will necessarily struggle mightily in gauging whether their creations are ultimately frogs or princes.
The best way to get better at judging our ideas is to gather feedback. Put a lot of ideas out there and see which ones are praised and adopted by your target audience. After spending decades creating comedy, The Daily Show cocreator Lizz Winstead still doesn’t know what will make people laugh. She recalls that she was “desperately trying to figure out jokes, writing them out, and trying them on stage.” Some of them sizzled; others popped. Now, with social media, she has a more rapid feedback mechanism. When she thinks of a joke, she tweets it; when she comes up with a longer bit, she posts it on Facebook. When she receives at least twenty-five retweets in less than a minute, or a high number of Facebook shares, she saves the idea. At the end of the day, she develops the material that proved most popular with her audience. “Twitter and Facebook have tremendously helped me decide what people care about,” Winstead explains.
When developing the Segway, Dean Kamen didn’t open the door to this kind of feedback. Concerned that someone would steal his idea, or that the fundamental concept would become public too soon, he maintained strict secrecy rules. Many of his own employees weren’t allowed access to the area where the Segway was being developed; only an elite group of potential investors had a chance to try it out. When building the Segway, his team generated a wide number of ideas, but didn’t have enough critical input from customers to make the right choices for the final product. The device went through three or four iterations before a customer ever saw it.* Conviction in our ideas is dangerous not only because it leaves us vulnerable to false positives, but also because it stops us from generating the requisite variety to reach our creative potential.
But Kamen and his team weren’t the only ones who were too bullish on the Segway. Where did virtuosos like Steve Jobs, Jeff Bezos, and John Doerr misstep in their judgments about the device? To find out, let’s first take a look at why many executives and test audiences failed to see the potential in Seinfeld.
Prisoners of Prototypes and Parochial Preferences
When the first Seinfeld script was submitted, executives didn’t know what to do with it. It was “totally unconventional,” NBC executive Warren Littlefield said. “It didn’t sound like anything else on television. There was no historical precedent.”
In Justin Berg’s study of circus performances, although circus managers made more accurate forecasts than artists, they still weren’t very good, especially regarding the most novel acts. Managers tend to be too risk averse: they focus on the costs of investing in bad ideas rather than the benefits of piloting good ones, which leads them to commit a large number of false negatives. The author of the initial report on the Seinfeld pilot felt it was on the border between “weak” and “moderate.” He was leaning toward moderate, but his boss overruled him and rated it weak.
These types of false negatives are common in the entertainment industry. Studio executives passed on hits ranging from Star Wars to E.T. to Pulp Fiction. In publishing, managers rejected The Chronicles of Narnia, The Diary of Anne Frank, Gone with the Wind, Lord of the Flies, and Harry Potter—as of 2015, J. K. Rowling’s books alone had brought in over $25 billion and sold more copies than any book series, ever. The annals of corporate innovation are filled with tales of managers ordering employees to stop working on projects that turned out to be big hits, from Nichia’s invention of LED lighting to Pontiac’s Fiero car to HP’s electrostatic displays. The Xbox was almost buried at Microsoft; the laser printer was nearly canceled at Xerox for being expensive and impractical.
In the face of uncertainty, our first instinct is often to reject novelty, looking for reasons why unfamiliar concepts might fail. When managers vet novel ideas, they’re in an evaluative mindset. To protect themselves against the risks of a bad bet, they compare the new notion on the table to templates of ideas that have succeeded in the past. When publishing executives passed on Harry Potter, they said it was too long for a children’s book; when Brandon Tartikoff saw the Seinfeld pilot, he felt it was “too Jewish” and “too New York” to appeal to a wide audience.
Rice professor Erik Dane finds that the more expertise and experience people gain, the more entrenched they become in a particular way of viewing the world. He points to studies showing that expert bridge players struggled more than novices to adapt when the rules were changed, and that expert accountants were worse than novices at applying a new tax law. As we gain knowledge about a domain, we become prisoners of our prototypes.
In principle, audiences should be more open to novelty than managers. They don’t have the blinders associated with expertise, and they have little to lose by considering a fresh format and expressing enthusiasm for an unusual idea. In practice, though, Justin Berg finds that test audiences are no better than managers at predicting the success of new ideas: focus groups are effectively set up to make the same mistakes as managers.
When you watch a show in your living room, you get absorbed in the plot. If you find yourself laughing throughout, you’ll end up pronouncing it funny. When you watch it in a focus group, however, you don’t engage with the program in the same way. You’re conscious of the fact that you’re there to evaluate it, not experience it, so you’re judging it from the start. Because you’re trying to figure out whether people will watch it, you naturally assess it against established ideas of how such a show ought to work. When test audiences viewed the Seinfeld pilot, they thought it lacked the community of Cheers, the family dynamics of The Cosby Show, and the relatability of ALF. It was all too easy to find flaws in a show that was ostensibly about nothing.
“The truth is, most pilots don’t test well,” Warren Littlefield observes, because “audiences do not respond well to things that are new or different.” Audiences don’t have enough experience: they simply haven’t seen a lot of the novel ideas that landed on the cutting-room floor. “The Seinfeld testing should put an end to all conversations about testing, ever. Please don’t tell me my show is going to come down to twenty people in Sherman Oaks,” comedian Paul Reiser says. “I’ve never been to any testing that’s any good.”
So neither test audiences nor managers are ideal judges of creative ideas. They’re too prone to false negatives; they focus too much on reasons to reject an idea and stick too closely to existing prototypes. And we’ve seen that creators struggle as well, because they’re too positive about their own ideas. But there is one group of forecasters that does come close to attaining mastery: fellow creators evaluating one another’s ideas. In Berg’s study of circus acts, the most accurate predictors of whether a video would get liked, shared, and funded were peers evaluating one another.
When artists assessed one another’s performances, they were about twice as accurate as managers and test audiences in predicting how often the videos would be shared. Compared to creators, managers and test audiences were 56 percent and 55 percent more prone to major false negatives, undervaluing a strong, novel performance by five ranks or more in the set of ten they viewed.
We often speak of the wisdom of crowds, but we need to be careful about which crowds we’re considering. On average, the combined forecasts of all 120 circus managers were no better than a typical single creator’s predictions. Managers and test audiences tended to fixate on a particular category of favored acts and reject the rest. Creators were more open to different kinds of performances—they saw potential in peers who did aerial and ground acrobatics, but also in skilled jugglers and mimes.*
Instead of attempting to assess our own originality or seeking feedback from managers, we ought to turn more often to our colleagues. They lack the risk-aversion of managers and test audiences; they’re open to seeing the potential in unusual possibilities, which guards against false negatives. At the same time, they have no particular investment in our ideas, which gives them enough distance to offer an honest appraisal and protects against false positives.
This evidence helps to explain why many performers enjoy the approval of audiences but covet the admiration of their peers. Comedians often say that the highest badge of honor is to make a fellow comic laugh; magicians like fooling audiences but live to baffle their brethren. The usual explanation for this preference is status striving: we crave acceptance by our peer group, those similar to us. But Berg’s research suggests that we’re also drawn to peer evaluations because they provide the most reliable judgments.
When we evaluate new ideas, we can become better at avoiding false negatives by thinking more like creators. In a series of experiments, Berg asked over a thousand adults to make forecasts about the success of novel products in the marketplace. Some were ideas that might be useful—a 3-D image projector, a flooring system that simulates natural ground, and an automatic bed-maker. Others were less practical, like an electrified tablecloth to prevent ants from ruining a picnic. The rest were conventional ideas that varied in usefulness, from a portable container for steaming food in a microwave to a hands-free system for carrying towels.
Berg wanted to boost the chances that people would correctly rank a novel, useful idea first, as opposed to favoring conventional ideas. He randomly assigned half of the participants to think like managers by spending six minutes making a list of three criteria for evaluating the success of new products. This group then made the right bet on a novel, useful idea 51 percent of the time. But the other group of participants was much more accurate, choosing the most promising new idea over 77 percent of the time. All it took was having them spend their initial six minutes a little differently: instead of adopting a managerial mindset for evaluating ideas, they got into a creative mindset by generating ideas themselves. Just spending six minutes developing original ideas made them more open to novelty, improving their ability to see the potential in something unusual.
From these findings you might think that we can improve idea selection simply by making sure that managers have some experience as creators. But in Berg’s circus data, former artists who become managers aren’t significantly better in their evaluations than regular managers; pure artists are still the best forecasters. Once you take on a managerial role, it’s hard to avoid letting an evaluative mindset creep in to cause false negatives. Berg demonstrated this in an experiment by asking people to generate product ideas and then come up with a list of evaluation criteria, and subsequently measured the success of the ideas with an actual audience. Thinking like creators and then donning the manager hat dropped their forecasting accuracy to 41 percent.
When Berg reversed the order, so that they made a list of evaluation criteria first and then generated ideas, their accuracy climbed to 65 percent. If we want to increase our odds of betting on the best original ideas, we have to generate our own ideas immediately before we screen others’ suggestions. And this, it turns out, helps to explain why Seinfeld saw the light of day.
The Double-Edged Sword of Experience
When the test audience panned the Seinfeld pilot, “it was a dagger to the heart,” Warren Littlefield reflected. “We were afraid to go forward with something that was so strongly rejected by research.” Although the ideal people to give the show a chance might have been fellow comedy writers, there weren’t any pure writers in positions of power. But Rick Ludwin, the man who ultimately made the show happen, was the next best thing.
Ludwin would later make his mark by standing up for Jay Leno, going to bat for Conan O’Brien, and fighting to renew a show that didn’t have a big enough following early on: The Office. But his greatest contribution to television was commissioning the pilot for Seinfeld.
At that point in his career, Rick Ludwin didn’t even work in the comedy department, but handled variety and specials. When the Seinfeld pilot didn’t take off, he went on a mission to give it another chance. He found a few hours in his lineup that hadn’t been assigned, divided them into half-hour slots, and took the money from his specials budget to fund more episodes. “As far as we know, that was the smallest order of episodes ever for a television show,” Ludwin says. Jerry Seinfeld would go on to remark that an order of six episodes “is like a slap in the face.” NBC ordered just four.
“If you’re gonna make connections which are innovative,” Steve Jobs said back in 1982, “you have to not have the same bag of experience as everyone else does.” Working outside the sitcom department may have been Rick Ludwin’s greatest advantage. “Larry [David] and Jerry had never written a sitcom, and my department had never developed one,” Ludwin recalls. “We were a good match, because we didn’t know what rules we weren’t supposed to break.” His outsider status gave him enough detachment from the standard format of sitcoms to consider something different. Most sitcoms shot a few continuous scenes in a tidy 22-minute episode; Seinfeld often left conflicts unresolved and had as many as twenty different scenes packed in. This was bothersome if you lived exclusively in the sitcom world, but perfectly comfortable for a guy who used a different arrangement in every special.
At the same time, Ludwin did have the requisite experience in creating comedy. While working as a producer in the 1970s, he wrote jokes and sold them to Bob Hope, and then produced segments for a daytime variety show that featured comedy sketches. “Being around comedy writers is like going to a baseball fantasy camp. You think you’re really good, until you get up to bat,” he reminisces. “Not only can’t you hit the ball; you can’t even see the ball. I knew I was not in their league, but I at least spoke the same language.”
It is when people have moderate expertise in a particular domain that they’re the most open to radically creative ideas. Ludwin’s deep experience in comedy gave him the necessary expertise in humor; his broad experience outside sitcoms prevented him from getting blinded to alternative ways of delivering it. Instead of narrowly scrutinizing what made a sitcom a hit, he had cast a wider net in studying what made comedy in general succeed:
	You never know where the next big hit is coming from. It can come from left field. If you think, “That can’t possibly work because that producer doesn’t have enough experience, or no idea like that has ever worked”—if you have those kinds of roadblocks in your head, you’re going to miss something. One of the best things that I had going for me was the fact that I had never developed a primetime situation comedy, but I was accustomed to offbeat, off-kilter ideas. I could see what worked, and what didn’t work. The time I spent reading Saturday Night Live scripts made me more open to the offbeat storylines that are now legendary on Seinfeld.
This unique combination of broad and deep experience is critical for creativity. In a recent study comparing every Nobel Prize–winning scientist from 1901 to 2005 with typical scientists of the same era, both groups attained deep expertise in their respective fields of study. But the Nobel Prize winners were dramatically more likely to be involved in the arts than less accomplished scientists. Here’s what a team of fifteen researchers at Michigan State University found about engagement in the arts among Nobel Prize winners relative to ordinary scientists:
 
Artistic hobby	Odds for Nobel Prize winners relative to typical scientists
Music: playing an instrument, composing, conducting	2x greater
Arts: drawing, painting, printmaking, sculpting	7x greater
Crafts: woodworking, mechanics, electronics, glassblowing	7.5x greater
Writing: poetry, plays, novels, short stories, essays, popular books	12x greater
Performing: amateur actor, dancer, magician	22x greater
A representative study of thousands of Americans showed similar results for entrepreneurs and inventors. People who started businesses and contributed to patent applications were more likely than their peers to have leisure time hobbies that involved drawing, painting, architecture, sculpture, and literature.
Interest in the arts among entrepreneurs, inventors, and eminent scientists obviously reflects their curiosity and aptitude. People who are open to new ways of looking at science and business also tend to be fascinated by the expression of ideas and emotions through images, sounds, and words.* But it’s not just that a certain kind of original person seeks out exposure to the arts. The arts also serve in turn as a powerful source of creative insight.
When Galileo made his astonishing discovery of mountains on the moon, his telescope didn’t actually have enough magnifying power to support that finding. Instead, he recognized the zigzag pattern separating the light and dark areas of the moon. Other astronomers were looking through similar telescopes, but only Galileo “was able to appreciate the implications of the dark and light regions,” Simonton notes. He had the necessary depth of experience in physics and astronomy, but also breadth of experience in painting and drawing. Thanks to artistic training in a technique called chiaroscuro, which focuses on representations of light and shade, Galileo was able to detect mountains where others did not.
Just as scientists, entrepreneurs, and inventors often discover novel ideas through broadening their knowledge to include the arts, we can likewise gain breadth by widening our cultural repertoires. Research on highly creative adults shows that they tended to move to new cities much more frequently than their peers in childhood, which gave them exposure to different cultures and values, and encouraged flexibility and adaptability. In a recent study, a team of researchers led by strategy professor Frédéric Godart explored whether creativity might be influenced by time spent abroad. They focused on the fashion industry, tracking ratings from buyers and fashion critics of the creativity of collections produced by hundreds of fashion houses over the course of twenty-one seasons. The researchers studied the creative directors’ biographies, tracking the international experiences of industry icons like Giorgio Armani, Donna Karan, Karl Lagerfeld, Donatella Versace, and Vera Wang.
The most creative fashion collections came from houses where directors had the greatest experience abroad, but there were three twists. First, time living abroad didn’t matter: it was time working abroad, being actively engaged in design in a foreign country, that predicted whether their new collections were hits. The most original collections came from directors who had worked in two or three different countries.
Second, the more the foreign culture differed from that of their native land, the more that experience contributed to the directors’ creativity. An American gained little from working in Canada, compared to the originality dividends of a project in Korea or Japan.
But working in multiple countries with different cultures wasn’t enough. The third and most important factor was depth—the amount of time spent working abroad. A short stint did little good, because directors weren’t there long enough to internalize the new ideas from the foreign culture and synthesize them with their old perspectives. The highest originality occurred when directors had spent thirty-five years working abroad.
Rick Ludwin’s experience lines up with this model. He had depth from spending well over a decade working on different comedy sketches. He had breadth from living in the television equivalents of several very different foreign countries: variety and specials, and daytime and late-night talk shows. Having become fluent in multiple television languages, he saw promise where others had doubts. Once he got Seinfeld approved, Ludwin continued to oversee the show for the entire series, and he bet on writers who had the same insider-outsider status that he did. Almost all came from late night, and most had never worked on a sitcom before Seinfeld, which meant “there was never a problem with offbeat ideas.”*
The Hazards of Intuition: Where Steve Jobs Went Wrong
The first time Steve Jobs stepped on a Segway, he refused to climb off. When Dean Kamen gave other potential investors a turn, Jobs begrudgingly handed it over, but soon cut in. Jobs invited Kamen over for dinner, and as journalist Steve Kemper tells it, Jobs “thought the machine was as original and enthralling as the PC, and felt he had to be involved.”
Steve Jobs was famous for making big bets based on intuition rather than systematic analysis. Why was he right so often in software and hardware, but wrong this time? Three major forces left him overconfident about the Segway’s potential: domain inexperience, hubris, and enthusiasm.
Let’s start with experience. Whereas many NBC executives were too experienced in traditional sitcoms to appreciate the unorthodox genius of Seinfeld, the Segway’s early investors had the opposite problem: they didn’t know enough about transportation. Jobs specialized in the digital world, Jeff Bezos was the king of internet retail, and John Doerr had made his fortune investing in software and internet companies like Sun Microsystems, Netscape, Amazon, and Google. They had all been originals in their respective arenas, but being a creator in one particular area doesn’t make you a great forecaster in others. To accurately predict the success of a novel idea, it’s best to be a creator in the domain you’re judging.
New research led by Erik Dane shows us why: our intuitions are only accurate in domains where we have a lot of experience. In one experiment, people looked at ten designer handbags and judged whether they were real or fake. Half the participants had only five seconds to guess, which forced them to rely on their gut feelings. The other half had thirty seconds, which allowed them to inspect and analyze the features. Dane’s team also measured their handbag experience—some had a lot, owning more than three handbags made by Coach or Louis Vuitton, whereas others had never touched a designer bag.
If you’re the proud owner of several designer handbags, the less time you have to inspect them, the more accurate your judgments. Experienced handbag owners were 22 percent more accurate when they had just five seconds than when they had thirty seconds. When you’ve spent years studying handbags, intuition can beat analysis, because your unconscious mind excels at pattern recognition. If you stop and take the time to think, it’s easy to lose the forest in the trees.
If you don’t know anything about handbags, however, your intuition isn’t going to help you. In dealing with unfamiliar products, you need to take a step back and assess them. Non-experts make sounder judgments when they conduct a thorough analysis. When Steve Jobs had a gut feeling that the Segway would change the world, he was seduced by an impulsive attraction to its novelty rather than a careful examination of its usefulness. Harvard psychologist Teresa Amabile, one of the world’s foremost authorities on creativity, reminds us that to be successfully original, an invention needs to be new—but it also has to be practical. In a digital world dominated by invisible bits and bytes, Jobs was enamored with the possibility that the next breakthrough innovation would be in transportation. The Segway was an engineering marvel, and riding it was a thrill. “It was like a magic carpet. It was transformational as a product,” says Bill Sahlman, the Harvard entrepreneurship professor who introduced Kamen to Doerr. “But products don’t create value. Customers do.”
For a group of people with no transportation experience, a lot of homework was required to figure out whether the Segway was actually practical. One of the few investors to raise concerns in that area was Aileen Lee, then an associate partner working for Doerr at Kleiner Perkins. In board meetings, Lee asked questions about how the Segway would be used. How would you lock it? Where would you store groceries? She had another big practical concern: the price tag, since “five or eight thousand dollars is a lot of money for a normal person.” Looking back, she says, “I should have stood up more and said, ‘We aren’t getting this right.’”
Another early skeptic was Randy Komisar, who had been an entrepreneur, senior counsel at Apple, CEO at LucasArts Entertainment, and a founding board member at TiVo. “I think about it as if I was in that seat with the entrepreneur. I don’t think I’m smarter than those guys, but I saw something differently than they did. I think they saw a brilliant technology in an application that seemed extremely novel. When we got on the machines that day, it was a magical experience to be on two wheels, self-balancing, moving around,” Komisar recalls. “That first impression was a ‘wow’ experience. Now, why did I not get convinced by that?”
When Komisar scrutinized the market, he saw that the Segway wasn’t likely to replace the car but would be a substitute for walking or riding a bike. He didn’t view it as a product for ordinary consumers. “It’s a huge change of behavior at a very high expense with limited value beyond the wow factor for anybody with two feet,” he explains. Even if it was approved for use on sidewalks (which was still an open question), and the price point became affordable, it would take years to get people on board. He suggested instead focusing on the device’s usefulness for golf courses, postal services, police departments, and Disney parks. “You could see that there was a cost-value tradeoff in those applications, where this might have some advantages.” But Komisar continued to have serious reservations:
	I still looked at it as a very significant change of behavior at a very significant cost. It wasn’t clear to me that this would improve a mail carrier’s productivity, or that that was the goal for the postal service, which was pretty much hamstrung by labor-union contracts. On the golf course, people drive electric carts around all day long. Why will they use this instead?
Jobs, meanwhile, stuck to his intuition about novelty: “If enough people see the machine, you won’t have to convince them to architect cities around it. People are smart, and it’ll happen.”
As Nobel Prize–winning psychologist Daniel Kahneman and decision expert Gary Klein explain, intuitions are only trustworthy when people build up experience making judgments in a predictable environment. If you’re confronting a patient’s symptoms as a doctor or entering a burning building as a firefighter, experience will make your intuitions more accurate. There’s a stable, robust relationship between the patterns you’ve seen before and what you encounter today. But if you’re a stockbroker or political forecaster, the events of the past don’t have reliable implications for the present. Kahneman and Klein review evidence that experience helps physicists, accountants, insurance analysts, and chess masters—they all work in fields where cause-and-effect relationships are fairly consistent. But admissions officers, court judges, intelligence analysts, psychiatrists, and stockbrokers didn’t benefit much from experience. In a rapidly changing world, the lessons of experience can easily point us in the wrong direction. And because the pace of change is accelerating, our environments are becoming ever more unpredictable. This makes intuition less reliable as a source of insight about new ideas and places a growing premium on analysis.
Given that Jobs hadn’t accumulated the relevant experience in transportation, why was he so trusting of his intuition? This brings us to the second factor: “There’s a hubris that comes with success,” Komisar explains. Had he pushed back harder about his concerns, “Steve Jobs would’ve probably said, ‘You just don’t get it.’” Research in the transportation and airline industries backs Komisar up. The more successful people have been in the past, the worse they perform when they enter a new environment. They become overconfident, and they’re less likely to seek critical feedback even though the context is radically different. Jobs was in one of these success traps: with his track record and his history of proving naysayers wrong, he didn’t bother to check his intuition by gathering input from enough creators with relevant domain knowledge. And his intuition led him further astray when he encountered Dean Kamen’s presentation style.
The Perils of Passion
When Kamen pitched the Segway, he spoke passionately about how developing nations like China and India were building cities the size of New York every year. These urban centers were going to become clogged with cars, which are bad for our environment; the Segway could solve this problem. “He’s a force of nature,” Aileen Lee remembers. “He’s technical, experienced, and superpassionate about these issues, so he is transfixing.”
In a study led by Northeastern University entrepreneurship professor Cheryl Mitteness, more than five dozen angel investors made over 3,500 evaluations of entrepreneurs’ pitches and decided whether or not to fund them. The investors filled out a survey about whether their own styles were more intuitive or analytical, rated each entrepreneur’s passion and enthusiasm, and then evaluated the funding potential of each startup. The results showed that the more intuitive investors were, the greater their odds of being swayed by an entrepreneur’s passion.
As Daniel Kahneman explains in Thinking, Fast and Slow, intuition operates rapidly, based on hot emotions, whereas reason is a slower, cooler process. Intuitive investors are susceptible to getting caught up in an entrepreneur’s enthusiasm; analytical investors are more likely to focus on the facts and make cold judgments about the viability of the business. Jobs’s intuitive style predisposed him to get swept away by Kamen’s passion and the inherent novelty of the technology. And his hubris and inexperience with transportation left him vulnerable to trusting what would eventually be revealed as a false positive.
When assessing the prospects of a novel idea, it’s all too easy to be seduced by the enthusiasm of the people behind it. In the words of Google executives Eric Schmidt and Jonathan Rosenberg, “Passionate people don’t wear their passion on their sleeves; they have it in their hearts.” The passion to see an idea to fruition isn’t visible in the emotion people express. The enthusiasm we inject into our words, tone of voice, and body language isn’t a clue to the internal passion we experience, but merely a reflection of our presentation skills and our personalities. For example, research shows that extraverts tend to be more expressive than introverts, which means that they display more passion. But whether we tend to be more extraverted or introverted has essentially no bearing on whether we’ll succeed as entrepreneurs. You can love an idea and be determined to succeed, but still communicate it in a reserved manner.
This isn’t to say that passion is irrelevant to entrepreneurial success. There’s plenty of evidence that passionate entrepreneurs are able to grow their ventures faster and more successfully. For his part, Kamen was lacking the element of passion that helps ideas grow from invention to impact. Instead of being enticed by his passion for creating the Segway, the early investors should have assessed his passion for building a company and bringing the product successfully to market. And to do that, they shouldn’t have only paid attention to what he said. They should have also examined what he did.
After studying his history, Randy Komisar concluded that Kamen was a more impressive inventor than entrepreneur. In the past, Kamen’s most successful inventions were a response to customers coming to him with a problem to solve. In the 1970s, he came up with the idea for the portable drug infusion pump when his brother, a doctor, lamented that nurses were constantly delivering medications by hand that should be automated, and many patients were stuck in hospitals when they could be receiving medication at home. In the 1980s, Kamen developed the portable dialysis machine after Baxter Healthcare hired his company to refine its kidney dialysis for diabetic patients. He excelled at creating brilliant solutions to problems identified by others, not in finding the right problems to solve. In the case of the Segway, he started with a solution and then went hunting for a problem. Rather than responding to market pull, he made the mistake of initiating a technology push.
Although Kamen was passionate about the Segway, he wasn’t prepared to execute it successfully. If we want to improve our idea selection skills, we shouldn’t look at whether people have been successful. We need to track how they’ve been successful. “When we looked at Dean, we saw a credible founder who had a history of inventing successful medical devices, and the people he’d partnered with to make those products were still with him,” Aileen Lee says. “But when it came down to actually making the product, the day-to-day execution and making the product cost-effective was important.” Kamen didn’t have that experience. Bill Sahlman adds: “It’s never the idea; it’s always the execution.”
If we want to forecast whether the originators of a novel idea will make it successful, we need to look beyond the enthusiasm they express about their ideas and focus on the enthusiasm for execution that they reveal through their actions. Rick Ludwin didn’t bet on Jerry Seinfeld and Larry David because they looked or sounded passionate when they pitched their script—or even because they were genuinely excited about their concept for a show. He gave them a chance because he watched them revise their concept and observed their ability to get the execution right. “They were the kinds of guys who would be in the writer’s room trying to figure out how to fix the second act at midnight. You saw how meticulous Jerry was in his work. That’s the passion you’re looking for.”
Corrective Lenses for Idea Selection
My personal failure to invest in Warby Parker was a major false negative. After reading the research on idea selection, I recognized one of my limitations: I wasn’t a fellow creator in the domain, nor was I a customer. At first, I blamed my poor creative forecasting on my 20/20 vision. When you’ve never owned a pair of glasses, it’s awfully hard to gauge the preferences of people who do. But upon reflection, I realized that what I truly lacked was breadth. I had spent two years doing research and consulting for an eye-care company whose primary source of revenue was selling glasses in-store after optometrists provided prescriptions and customers tried them on. I was stuck in the default mindset of how glasses were traditionally bought and sold. Had I spent time generating ideas before hearing the pitch, or reading up on how other clothing and accessory products were sold online, I might have been more open to the idea.
The four founders weren’t hindered by these blinders: they had the right depth and breadth of experience. Three of them wore glasses and brought combined experience in bioengineering, health care, consulting, and banking. One of them, Dave, had lost his glasses while spending several months traveling abroad with no phone. When he came back to the United States, the need to buy a phone and glasses at the same time gave him a fresh perspective. Neil Blumenthal didn’t wear glasses, but he had spent the past five years working at a nonprofit that trained women to start businesses in Asia, Africa, and Latin America. The product that he taught women to sell was eyeglasses. That gave him the necessary depth of knowledge in the optical industry: he knew that glasses could be manufactured, designed, and sold at a lower cost. And having spent time outside the conventional eyeglass channels, he had the breadth to adopt a fresh approach. “It’s rare that originality comes from insiders,” Neil tells me, “especially when they’re as entrenched and comfortable as the optical industry.”
Because of the diversity of their experience, the Warby Parker entrepreneurs weren’t hampered by existing prototypes or limited by evaluative mindsets. Instead of assuming that their idea would work and going into full-on enthusiastic sales mode like Kamen did, they first sought extensive feedback from fellow creators and potential customers. By cutting out the middleman of the retailer, they had determined that they could sell pairs of glasses that normally cost $500 for $45. After a marketing expert warned them that their costs would increase—and that price was viewed as a sign of quality—they created a survey with mock product pages, randomly assigning customers to different price points. They found that the likelihood of purchase increased up to prices around $100, then plateaued and dropped in higher ranges. They tested different website designs with friends to see what would generate not only the most clicks, but also the strongest trust.
Since other companies could sell glasses online, the founders realized that branding would be critical to their success. To name the company, they spent six months generating ideas, building a spreadsheet with more than two thousand potential names. They tested their favorites in surveys and focus groups, finding that the Kerouac-inspired name of Warby Parker sounded sophisticated and unique, and evoked no negative associations. Then they brought passion for execution in spades.
Much of Warby Parker’s recent success is due to the way they involved peers in evaluating ideas. In 2014, they created a program called Warbles, inviting everyone in the company to submit suggestions and requests for new technology features at any time. Before Warbles was introduced, they had received ten to twenty idea submissions per quarter. With the new program, the number of submissions jumped to nearly four hundred as employees trusted that the idea selection process was meritocratic. One of the suggestions led to the company’s overhauling how they conducted retail sales; another led to a new booking system for appointments. “Neil and Dave are really brilliant,” says Warby Parker’s chief technology officer, Lon Binder, “but there’s no way they can be as brilliant as two hundred people combined.”
Instead of limiting access to the ideas and leaving it up to managers to decide which ones to pursue and implement, Warby Parker made the suggestions completely transparent in a Google document. Everyone in the company could read them, comment on them online, and discuss them in a biweekly meeting. This means that, just as Justin Berg recommends, the ideas are evaluated not only by managers, but also by fellow creators—who tend to be more open to radically novel ideas. The time employees spend generating ideas makes them better at discerning which suggestions from their colleagues are worthwhile.
The technology teams have full discretion to sort through the requests and start working on the ones that interest them. It sounds like a democracy, but there’s one twist: to give employees some guidance on which suggestions represent strategic priorities for the company, managers vote the promising ones up and the bad ones down. To avoid false positives and false negatives, the votes aren’t binding. Technology teams can overrule managers by selecting a request that didn’t receive a lot of votes and work to prove its value. “They don’t wait for permission to start building something,” says applied psychology expert Reb Rebele, who has worked on a study at Warby Parker. “But they gather feedback from peers before rolling things out to customers. They start fast and then slow down.”
Had the Segway been submitted to the Warbles process, a lot more critical feedback might have rolled in to prevent it from being made—or to generate a more useful design. Before it was too late, Dean Kamen would have learned to make it practical or licensed the technology to someone who could.
The Segway may have failed, but Kamen is still a brilliant inventor, Jeff Bezos is still a visionary entrepreneur, and John Doerr is still a shrewd investor. Whether you’re generating or evaluating new ideas, the best you can do is measure success on the kind of yardstick that batters use in baseball. As Randy Komisar puts it, “If I’m hitting .300, I’m a genius. That’s because the future cannot be predicted. The sooner you learn it, the sooner you can be good at it.”
Dean Kamen has moved on to unveil a series of new inventions, back in the health-care space where he made his original mark. There’s a prosthetic arm with advanced robotics technology that makes it possible for a soldier or amputee to pick up a grape and operate a hand drill—nicknamed “Luke” after a Star Wars scene in which Skywalker gets a bionic arm. There’s a new Stirling engine, a quiet, fuel-efficient machine for generating power and heating water. That engine powers the Slingshot water purifier, which can distill drinking water from any source, needs no filter, and can run on cow dung as a source of fuel. Kamen came full circle when he pitched the Slingshot to Komisar. Once again, though, Komisar is skeptical. Having traipsed around the developing world with a backpack himself, he thinks the machinery is too complicated for off-the-grid installations; when it stops working, it will wind up in a garbage heap. Whether this is an accurate forecast or a false negative remains to be seen.
As an inventor, Kamen’s best bet is to blindly generate novel ideas and gather more feedback from fellow creators to hone his vision about which ones might prove useful. As an investor, you’ll be able to see more clearly, but you’ll still be making one-eyed gambles. Instead of banking on a single idea, your wisest move might be to bet on a whole portfolio of Kamen’s creations.
In 2013 alone, over three hundred thousand patents were granted in the United States. The chances that any one of these inventions will change the world are tiny. Individual creators have far better odds over a lifetime of ideas. When we judge their greatness, we focus not on their averages, but on their peaks.