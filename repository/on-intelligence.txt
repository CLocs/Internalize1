[Source]
PROLOGUE
This book and my life are animated by two passions.
For twenty-five years I have been passionate about mobile computing. In the high-tech world of Silicon Valley, I am known for starting two companies, Palm Computing and Handspring, and as the architect of many handheld computers and cell phones such as the PalmPilot and the Treo.
But I have a second passion that predates my interest in computers—one I view as more important. I am crazy about brains. I want to understand how the brain works, not just from a philosophical perspective, not just in a general way, but in a detailed nuts and bolts engineering way. My desire is not only to understand what intelligence is and how the brain works, but how to build machines that work the same way. I want to build truly intelligent machines.
The question of intelligence is the last great terrestrial frontier of science. Most big scientific questions involve the very small, the very large, or events that occurred billions of years ago. But everyone has a brain. You are your brain. If you want to understand why you feel the way you do, how you perceive the world, why you make mistakes, how you are able to be creative, why music and art are inspiring, indeed what it is to be human, then you need to understand the brain. In addition, a successful theory of intelligence and brain function will have large societal benefits, and not just in helping us cure brainrelated diseases. We will be able to build genuinely intelligent machines, although they won’t be anything like the robots of popular fiction and computer science fantasy. Rather, intelligent machines will arise from a new set of principles about the nature of intelligence. As such, they will help us accelerate our knowledge of the world, help us explore the universe, and make the world safer. And along the way, a large industry will be created.
Fortunately, we live at a time when the problem of understanding intelligence can be solved. Our generation has access to a mountain of data about the brain, collected over hundreds of years, and the rate at which we are gathering more data is accelerating. The United States alone has thousands of neuroscientists. Yet we have no productive theories about what intelligence is or how the brain works as a whole. Most neurobiologists don’t think much about overall theories of the brain because they’re engrossed in doing experiments to collect more data about the brain’s many subsystems. And although legions of computer programmers have tried to make computers intelligent, they have failed. I believe they will continue to fail as long as they keep ignoring the differences between computers and brains.
What then is intelligence such that brains have it but computers don’t? Why can a six-year-old hop gracefully from rock to rock in a streambed while the most advanced robots of our time are lumbering zombies? Why are three-year-olds already well on their way to mastering language while computers can’t, despite half a century of programmers’ best efforts? Why can you tell a cat from a dog in a fraction of a second while a supercomputer cannot make the distinction at all? These are great mysteries waiting for an answer. We have plenty of clues; what we need now are a few critical insights.
You may be wondering why a computer designer is writing a book about brains. Or put another way, if I love brains why didn’t I make a career in brain science or in artificial intelligence? The answer is I tried to, several times, but I refused to study the problem of intelligence as others have before me. I believe the best way to solve this problem is to use the detailed biology of the brain as a constraint and as a guide, yet think about intelligence as a computational problem—a position somewhere between biology and computer science. Many biologists tend to reject or ignore the idea of thinking of the brain in computational terms, and computer scientists often don’t believe they have anything to learn from biology. Also, the world of science is less accepting of risk than the world of business. In technology businesses, a person who pursues a new idea with a reasoned approach can enhance his or her career regardless of whether the particular idea turns out to be successful. Many successful entrepreneurs achieved success only after earlier failures. But in academia, a couple of years spent pursuing a new idea that does not work out can permanently ruin a young career. So I pursued the two passions in my life simultaneously, believing that success in industry would help me achieve success in understanding the brain. I needed the financial resources to pursue the science I wanted, and I needed to learn how to affect change in the world, how to sell new ideas, all of which I hoped to get from working in Silicon Valley.
In August 2002 I started a research center, the Redwood Neuroscience Institute (RNI), dedicated to brain theory. There are many neuroscience centers in the world, but no others are dedicated to finding an overall theoretical understanding of the neocortex—the part of the human brain responsible for intelligence. That is all we study at RNI. In many ways, RNI is like a start-up company. We are pursuing a dream that some people think is unattainable, but we are lucky to have a great group of people, and our efforts are starting to bear fruit.
The agenda for this book is ambitious. It describes a comprehensive theory of how the brain works. It describes what intelligence is and how your brain creates it. The theory I present is not a completely new one. Many of the individual ideas you are about to read have existed in some form or another before, but not together in a coherent fashion. This should be expected. It is said that “new ideas” are often old ideas repackaged and reinterpreted. That certainly applies to the theory proposed here, but packaging and interpretation can make a world of difference, the difference between a mass of details and a satisfying theory. I hope it strikes you the way it does many people. A typical reaction I hear is, “It makes sense. I wouldn’t have thought of intelligence this way, but now that you describe it to me I can see how it all fits together.” With this knowledge most people start to see themselves a little differently. You start to observe your own behavior saying, “I understand what just happened in my head.” Hopefully when you have finished this book, you will have new insight into why you think what you think and why you behave the way you behave. I also hope that some readers will be inspired to focus their careers on building intelligent machines based on the principles outlined in these pages.
I often refer to this theory and my approach to studying intelligence as “real intelligence” to distinguish it from “artificial intelligence.” AI scientists tried to program computers to act like humans without first answering what intelligence is and what it means to understand. They left out the most important part of building intelligent machines, the intelligence! “Real intelligence” makes the point that before we attempt to build intelligent machines, we have to first understand how the brain thinks, and there is nothing artificial about that. Only then can we ask how we can build intelligent machines.
The book starts with some background on why previous attempts at understanding intelligence and building intelligent machines have failed. I then introduce and develop the core idea of the theory, what I call the memory-prediction framework. In chapter 6 I detail how the physical brain implements the memory-prediction model—in other words, how the brain actually works. I then discuss social and other implications of the theory, which for many readers might be the most thoughtprovoking section. The book ends with a discussion of intelligent machines—how we can build them and what the future will be like. I hope you find it fascinating. Here are some of the questions we will cover along the way:
	Can computers be intelligent?
	For decades, scientists in the field of artificial intelligence have claimed that computers will be intelligent when they are powerful enough. I don’t think so, and I will explain why. Brains and computers do fundamentally different things.
	Weren’t neural networks supposed to lead to intelligent machines?
	Of course the brain is made from a network of neurons, but without first understanding what the brain does, simple neural networks will be no more successful at creating intelligent machines than computer programs have been.
	Why has it been so hard to figure out how the brain works?
	Most scientists say that because the brain is so complicated, it will take a very long time for us to understand it. I disagree. Complexity is a symptom of confusion, not a cause. Instead, I argue we have a few intuitive but incorrect assumptions that mislead us. The biggest mistake is the belief that intelligence is defined by intelligent behavior.
	What is intelligence if it isn’t defined by behavior?
	The brain uses vast amounts of memory to create a model of the world. Everything you know and have learned is stored in this model. The brain uses this memory-based model to make continuous predictions of future events. It is the ability to make predictions about the future that is the crux of intelligence. I will describe the brain’s predictive ability in depth; it is the core idea in the book.
	How does the brain work?
	The seat of intelligence is the neocortex. Even though it has a great number of abilities and powerful flexibility, the neocortex is surprisingly regular in its structural details. The different parts of the neocortex, whether they are responsible for vision, hearing, touch, or language, all work on the same principles. The key to understanding the neocortex is understanding these common principles and, in particular, its hierarchical structure. We will examine the neocortex in sufficient detail to show how its structure captures the structure of the world. This discussion will be the most technical part of the book, but interested nonscientist readers should be able to understand it.
	What are the implications of this theory?
	This theory of the brain can help explain many things, such as how we are creative, why we feel conscious, why we exhibit prejudice, how we learn, and why “old dogs” have trouble learning “new tricks.” I will discuss a number of these topics. Overall, this theory gives us insight into who we are and why we do what we do.
	Can we build intelligent machines and what will they do?
	Yes. We can and we will. Over the next few decades, I see the capabilities of such machines evolving rapidly and in interesting directions. Some people fear that intelligent machines could be dangerous to humanity, but I argue strongly against this idea. We are not going to be overrun by robots. It will be far easier to build machines that outstrip our abilities in high-level thought such as physics and mathematics than to build anything like the walking, talking robots we see in popular fiction. I will explore the incredible directions in which this technology is likely to go.
My goal is to explain this new theory of intelligence and how the brain works in a way that anybody will be able to understand. A good theory should be easy to comprehend, not obscured in jargon or convoluted argument. I’ll start with a basic framework and then add details as we go. Some will be reasoning just on logical grounds; some will involve particular aspects of brain circuitry. Some of the details of what I propose are certain to be wrong, which is always the case in any area of science. A fully mature theory will take years to develop, but that doesn’t diminish the power of the core idea.
When I first became interested in brains many years ago, I went to my local library to look for a good book that would explain how brains worked. As a teenager I had become accustomed to being able to find well-written books that explained almost any topic of interest. There were books on relativity theory, black holes, magic, and mathematics—whatever I was fascinated with at the moment. Yet my search for a satisfying brain book turned up empty. I came to realize that no one had any idea how the brain actually worked. There weren’t even any bad or unproven theories; there simply were none. This was unusual. For example, at that time no one knew how the dinosaurs had died, but there were plenty of theories, all of which you could read about. There was nothing like this for brains. At first I had trouble believing it. It bothered me that we didn’t know how this critical organ worked. While studying what we did know about brains, I came to believe that there must be a straightforward explanation. The brain wasn’t magic, and it didn’t seem to me that the answers would even be that complex. The mathematician Paul Erdös believed that the simplest mathematical proofs already exist in some ethereal book and a mathematician’s job was to find them, to “read the book.” In the same way, I felt that the explanation of intelligence was “out there.” I could taste it. I wanted to read the book.
For the past twenty-five years, I have had a vision of that small, straightforward book on the brain. It was like a carrot keeping me motivated during those years. This vision has shaped the book you are holding in your hands right now. I have never liked complexity, in either science or technology. You can see that reflected in the products I have designed, which are often noted for their ease of use. The most powerful things are simple. Thus this book proposes a simple and straightforward theory of intelligence. I hope you enjoy it.
1
 ARTIFICIAL INTELLIGENCE
When I graduated from Cornell in June 1979 with a degree in electrical engineering, I didn’t have any major plans for my life. I started work as an engineer at the new Intel campus in Portland, Oregon. The microcomputer industry was just starting, and Intel was at the heart of it. My job was to analyze and fix problems found by other engineers working in the field with our main product, single board computers. (Putting an entire computer on a single circuit board had only recently been made possible by Intel’s invention of the microprocessor.) I published a newsletter, got to do some traveling, and had a chance to meet customers. I was young and having a good time, although I missed my college sweetheart who had taken a job in Cincinnati.
A few months later, I encountered something that was to change my life’s direction. That something was the newly published September issue of Scientific American, which was dedicated entirely to the brain. It rekindled my childhood interest in brains. It was fascinating. From it I learned about the organization, development, and chemistry of the brain, neural mechanisms of vision, movement, and other specializations, and the biological basis for disorders of the mind. It was one of the best Scientific American issues of all time. Several neuroscientists I’ve spoken to have told me it played a significant role in their career choice, just as it did for me.
The final article, “Thinking About the Brain,” was written by Francis Crick, the codiscoverer of the structure of DNA who had by then turned his talents to studying the brain. Crick argued that in spite of a steady accumulation of detailed knowledge about the brain, how the brain worked was still a profound mystery. Scientists usually don’t write about what they don’t know, but Crick didn’t care. He was like the boy pointing to the emperor with no clothes. According to Crick, neuroscience was a lot of data without a theory. His exact words were, “what is conspicuously lacking is a broad framework of ideas.” To me this was the British gentleman’s way of saying, “We don’t have a clue how this thing works.” It was true then, and it’s still true today.
Crick’s words were to me a rallying call. My lifelong desire to understand brains and build intelligent machines was brought to life. Although I was barely out of college, I decided to change careers. I was going to study brains, not only to understand how they worked, but to use that knowledge as a foundation for new technologies, to build intelligent machines. It would take some time to put this plan into action.
In the spring of 1980 I transferred to Intel’s Boston office to be reunited with my future wife, who was starting graduate school. I took a position teaching customers and employees how to design microprocessor-based systems. But I had my sights on a different goal: I was trying to figure out how to work on brain theory. The engineer in me realized that once we understood how brains worked, we could build them, and the natural way to build artificial brains was in silicon. I worked for the company that invented the silicon memory chip and the microprocessor; therefore, perhaps I could interest Intel in letting me spend part of my time thinking about intelligence and how we could design brainlike memory chips. I wrote a letter to Intel’s chairman, Gordon Moore. The letter can be distilled to the following:
	Dear Dr. Moore,
	I propose that we start a research group devoted to understanding how the brain works. It can start with one person—me—and go from there. I am confident we can figure this out. It will be a big business one day.
—Jeff Hawkins
Moore put me in touch with Intel’s chief scientist, Ted Hoff. I flew to California to meet him and lay out my proposal for studying the brain. Hoff was famous for two things. The first, which I was aware of, was for his work in designing the first microprocessor. The second, which I was not aware of at the time, was for his work in early neural network theory. Hoff had experience with artificial neurons and some of the things you could do with them. I wasn’t prepared for this. After listening to my proposal, he said he didn’t believe it would be possible to figure out how the brain works in the foreseeable future, and so it didn’t make sense for Intel to support me. Hoff was correct, because it is now twenty-five years later and we are just starting to make significant progress in understanding brains. Timing is everything in business. Still, at the time I was pretty disappointed.
I tend to seek the path of least friction to achieve my goals. Working on brains at Intel would have been the simplest transition. With that option eliminated I looked for the next best thing. I decided to apply to graduate school at the Massachusetts Institute of Technology, which was famous for its research on artificial intelligence and was conveniently located down the road. It seemed a great match. I had extensive training in computer science—“check.” I had a desire to build intelligent machines, “check.” I wanted to first study brains to see how they worked … “uh, that’s a problem.” This last goal, wanting to understand how brains worked, was a nonstarter in the eyes of the scientists at the MIT artificial intelligence lab.
It was like running into a brick wall. MIT was the mothership of artificial intelligence. At the time I applied to MIT, it was home to dozens of bright people who were enthralled with the idea of programming computers to produce intelligent behavior. To these scientists, vision, language, robotics, and mathematics were just programming problems. Computers could do anything a brain could do, and more, so why constrain your thinking by the biological messiness of nature’s computer? Studying brains would limit your thinking. They believed it was better to study the ultimate limits of computation as best expressed in digital computers. Their holy grail was to write computer programs that would first match and then surpass human abilities. They took an ends-justify-the-means approach; they were not interested in how real brains worked. Some took pride in ignoring neurobiology.
This struck me as precisely the wrong way to tackle the problem. Intuitively I felt that the artificial intelligence approach would not only fail to create programs that do what humans can do, it would not teach us what intelligence is. Computers and brains are built on completely different principles. One is programmed, one is self-learning. One has to be perfect to work at all, one is naturally flexible and tolerant of failures. One has a central processor, one has no centralized control. The list of differences goes on and on. The biggest reason I thought computers would not be intelligent is that I understood how computers worked, down to the level of the transistor physics, and this knowledge gave me a strong intuitive sense that brains and computers were fundamentally different. I couldn’t prove it, but I knew it as much as one can intuitively know anything.
Ultimately, I reasoned, AI might lead to useful products, but it wasn’t going to build truly intelligent machines.
In contrast, I wanted to understand real intelligence and perception, to study brain physiology and anatomy, to meet Francis Crick’s challenge and come up with a broad framework for how the brain worked. I set my sights in particular on the neocortex—the most recently developed part of the mammalian brain and the seat of intelligence. After understanding how the neocortex worked, then we could go about building intelligent machines, but not before.
Unfortunately, the professors and students I met at MIT did not share my interests. They didn’t believe that you needed to study real brains to understand intelligence and build intelligent machines. They told me so. In 1981 the university rejected my application.
Many people today believe that AI is alive and well and just waiting for enough computing power to deliver on its many promises. When computers have sufficient memory and processing power, the thinking goes, AI programmers will be able to make intelligent machines. I disagree. AI suffers from a fundamental flaw in that it fails to adequately address what intelligence is or what it means to understand something. A brief look at the history of AI and the tenets on which it was built will explain how the field has gone off course.
The AI approach was born with the digital computer. A key figure in the early AI movement was the English mathematician Alan Turing, who was one of the inventors of the idea of the general-purpose computer. His masterstroke was to formally demonstrate the concept of universal computation: that is, all computers are fundamentally equivalent regardless of the details of how they are built. As part of his proof, he conceived an imaginary machine with three essential parts: a processing box, a paper tape, and a device that reads and writes marks on the tape as it moves back and forth. The tape was for storing information—like the famous l’s and O’s of computer code (this was before the invention of memory chips or the disk drive, so Turing imagined paper tape for storage). The box, which today we call a central processing unit (CPU), follows a set of fixed rules for reading and editing the information on the tape. Turing proved, mathematically, that if you choose the right set of rules for the CPU and give it an indefinitely long tape to work with, it can perform any definable set of operations in the universe. It would be one of many equivalent machines now called Universal Turing Machines. Whether the problem is to compute square roots, calculate ballistic trajectories, play games, edit pictures, or reconcile bank transactions, it is all 1’s and 0’s underneath, and any Turing Machine can be programmed to handle it. Information processing is information processing is information processing. All digital computers are logically equivalent.
Turing’s conclusion was indisputably true and phenomenally fruitful. The computer revolution and all its products are built on it. Then Turing turned to the question of how to build an intelligent machine. He felt computers could be intelligent, but he didn’t want to get into arguments about whether this was possible or not. Nor did he think he could define intelligence formally, so he didn’t even try. Instead, he proposed an existence proof for intelligence, the famous Turing Test: if a computer can fool a human interrogator into thinking that it too is a person, then by definition the computer must be intelligent. And so, with the Turing Test as his measuring stick and the Turing Machine as his medium, Turing helped launch the field of AI. Its central dogma: the brain is just another kind of computer. It doesn’t matter how you design an artificially intelligent system, it just has to produce humanlike behavior.
The AI proponents saw parallels between computation and thinking. They said, “Look, the most impressive feats of human intelligence clearly involve the manipulation of abstract symbols—and that’s what computers do too. What do we do when we speak or listen? We manipulate mental symbols called words, using well-defined rules of grammar. What do we do when we play chess? We use mental symbols that represent the properties and locations of the various pieces. What do we do when we see? We use mental symbols to represent objects, their positions, their names, and other properties. Sure, people do all this with brains and not with the kinds of computers we build, but Turing has shown that it doesn’t matter how you implement or manipulate the symbols. You can do it with an assembly of cogs and gears, with a system of electronic switches, or with the brain’s network of neurons—whatever, as long as your medium can realize the functional equivalent of a Universal Turing Machine.”
This assumption was bolstered by an influential scientific paper published in 1943 by the neurophysiologist Warren McCulloch and the mathematician Walter Pitts. They described how neurons could perform digital functions—that is, how nerve cells could conceivably replicate the formal logic at the heart of computers. The idea was that neurons could act as what engineers call logic gates. Logic gates implement simple logical operations such as AND, NOT, and OR. Computer chips are composed of millions of logic gates all wired together into precise, complicated circuits. A CPU is just a collection of logic gates.
McCulloch and Pitts pointed out that neurons could also be connected together in precise ways to perform logic functions. Since neurons gather input from each other and process those inputs to decide whether to fire off an output, it was conceivable that neurons might be living logic gates. Thus, they inferred, the brain could conceivably be built out of AND-gates, OR-gates, and other logic elements all built with neurons, in direct analogy with the wiring of digital electronic circuits. It isn’t clear whether McCulloch and Pitts actually believed the brain worked this way; they only said it was possible. And, logically speaking, this view of neurons is possible. Neurons can, in theory, implement digital functions. However, no one bothered to ask if that was how neurons actually were wired in the brain. They took it as proof, irrespective of the lack of biological evidence, that brains were just another kind of computer.
It’s also worth noting that AI philosophy was buttressed by the dominant trend in psychology during the first half of the twentieth century, called behaviorism. The behaviorists believed that it was not possible to know what goes on inside the brain, which they called an impenetrable black box. But one could observe and measure an animal’s environment and its behaviors—what it senses and what it does, its inputs and its outputs. They conceded that the brain contained reflex mechanisms that could be used to condition an animal into adopting new behaviors through reward and punishments. But other than this, one did not need to study the brain, especially messy subjective feelings such as hunger, fear, or what it means to understand something. Needless to say, this research philosophy eventually withered away throughout the second half of the twentieth century, but AI would stick around a lot longer.
As World War II ended and electronic digital computers became available for broader applications, the pioneers of AI rolled up their sleeves and began programming. Language translation? Easy! It’s a kind of code breaking. We just need to map each symbol in System A onto its counterpart in System B. Vision? That looks easy too. We already know geometric theorems that deal with rotation, scale, and displacement, and we can easily encode them as computer algorithms—so we’re halfway there. AI pundits made grand claims about how quickly computer intelligence would first match and then surpass human intelligence.
Ironically, the computer program that came closest to passing the Turing Test, a program called Eliza, mimicked a psychoanalyst, rephrasing your questions back at you. For example, if a person typed in, “My boyfriend and I don’t talk anymore,” Eliza might say, “Tell me more about your boyfriend” or “Why do you think your boyfriend and you don’t talk anymore?” Designed as a joke, the program actually fooled some people, even though it was dumb and trivial. More serious efforts included programs such as Blocks World, a simulated room containing blocks of different colors and shapes. You could pose questions to Blocks World such as “Is there a green pyramid on top of the big red cube?” or “Move the blue cube on top of the little red cube.” The program would answer your question or try to do what you asked. It was all simulated—and it worked. But it was limited to its own highly artificial world of blocks. Programmers couldn’t generalize it to do anything useful.
The public, meanwhile, was impressed by a continuous stream of seeming successes and news stories about AI technology. One program that generated initial excitement was able to solve mathematical theorems. Ever since Plato, multistep deductive inference has been seen as the pinnacle of human intelligence, so at first it seemed that AI had hit the jackpot. But, like Blocks World, it turned out the program was limited. It could only find very simple theorems, which were already known. Then there was a large stir about “expert systems,” databases of facts that could answer questions posed by human users. For example, a medical expert system might be able to diagnose a patient’s disease if given a list of symptoms. But again, they turned out to be of limited use and didn’t exhibit anything close to generalized intelligence. Computers could play checkers at expert skill levels and eventually IBM’s Deep Blue famously beat Gary Kasparov, the world chess champion, at his own game. But these successes were hollow. Deep Blue didn’t win by being smarter than a human; it won by being millions of times faster than a human. Deep Blue had no intuition. An expert human player looks at a board position and immediately sees what areas of play are most likely to be fruitful or dangerous, whereas a computer has no innate sense of what is important and must explore many more options. Deep Blue also had no sense of the history of the game, and didn’t know anything about its opponent. It played chess yet didn’t understand chess, in the same way that a calculator performs arithmetic but doesn’t understand mathematics.
In all cases, the successful AI programs were only good at the one particular thing for which they were specifically designed. They didn’t generalize or show flexibility, and even their creators admitted they didn’t think like humans. Some AI problems, which at first were thought to be easy, yielded no progress. Even today, no computer can understand language as well as a three-year-old or see as well as a mouse.
After many years of effort, unfulfilled promises, and no unqualified successes, AI started to lose its luster. Scientists in the field moved on to other areas of research. AI start-up companies failed. And funding became scarcer. Programming computers to do even the most basic tasks of perception, language, and behavior began to seem impossible. Today, not much has changed. As I said earlier, there are still people who believe that AI’s problems can be solved with faster computers, but most scientists think the entire endeavor was flawed.
We shouldn’t blame the AI pioneers for their failures. Alan Turing was brilliant. They all could tell that the Turing Machine would change the world—and it did, but not through AI.
My skepticism of AI’s assertions was honed around the same time that I applied to MIT. John Searle, an influential philosophy professor at the University of California at Berkeley, was at that time saying that computers were not, and could not be, intelligent. To prove it, in 1980 he came up with a thought experiment called the Chinese Room. It goes like this:
Suppose you have a room with a slot in one wall, and inside is an English-speaking person sitting at a desk. He has a big book of instructions and all the pencils and scratch paper he could ever need. Flipping through the book, he sees that the instructions, written in English, dictate ways to manipulate, sort, and compare Chinese characters. Mind you, the directions say nothing about the meanings of the Chinese characters; they only deal with how the characters are to be copied, erased, reordered, transcribed, and so forth.
Someone outside the room slips a piece of paper through the slot. On it is written a story and questions about the story, all in Chinese. The man inside doesn’t speak or read a word of Chinese, but he picks up the paper and goes to work with the rulebook. He toils and toils, rotely following the instructions in the book. At times the instructions tell him to write characters on scrap paper, and at other times to move and erase characters. Applying rule after rule, writing and erasing characters, the man works until the book’s instructions tell him he is done. When he is finished at last he has written a new page of characters, which unbeknownst to him are the answers to the questions. The book tells him to pass his paper back through the slot. He does it, and wonders what this whole tedious exercise has been about.
Outside, a Chinese speaker reads the page. The answers are all correct, she notes—even insightful. If she is asked whether those answers came from an intelligent mind that had understood the story, she will definitely say yes. But can she be right? Who understood the story? It wasn’t the fellow inside, certainly; he is ignorant of Chinese and has no idea what the story was about. It wasn’t the book, which is just, well, a book, sitting inertly on the writing desk amid piles of paper. So where did the understanding occur? Searle’s answer is that no understanding did occur; it was just a bunch of mindless page flipping and pencil scratching. And now the bait-and-switch: the Chinese Room is exactly analogous to a digital computer. The person is the CPU, mindlessly executing instructions, the book is the software program feeding instructions to the CPU, and the scratch paper is the memory. Thus, no matter how cleverly a computer is designed to simulate intelligence by producing the same behavior as a human, it has no understanding and it is not intelligent. (Searle made it clear he didn’t know what intelligence is; he was only saying that whatever it is, computers don’t have it.)
This argument created a huge row among philosophers and AI pundits. It spawned hundreds of articles, plus more than a little vitriol and bad blood. AI defenders came up with dozens of counterarguments to Searle, such as claiming that although none of the room’s component parts understood Chinese, the entire room as a whole did, or that the person in the room really did understand Chinese, but just didn’t know it. As for me, I think Searle had it right. When I thought through the Chinese Room argument and when I thought about how computers worked, I didn’t see understanding happening anywhere. I was convinced we needed to understand what “understanding” is, a way to define it that would make it clear when a system was intelligent and when it wasn’t, when it understands Chinese and when it doesn’t. Its behavior doesn’t tell us this.
A human doesn’t need to “do” anything to understand a story. I can read a story quietly, and although I have no overt behavior my understanding and comprehension are clear, at least to me. You, on the other hand, cannot tell from my quiet behavior whether I understand the story or not, or even if I know the language the story is written in. You might later ask me questions to see if I did, but my understanding occurred when I read the story, not just when I answer your questions. A thesis of this book is that understanding cannot be measured by external behavior; as we’ll see in the coming chapters, it is instead an internal metric of how the brain remembers things and uses its memories to make predictions. The Chinese Room, Deep Blue, and most computer programs don’t have anything akin to this. They don’t understand what they are doing. The only way we can judge whether a computer is intelligent is by its output, or behavior.
The ultimate defensive argument of AI is that computers could, in theory, simulate the entire brain. A computer could model all the neurons and their connections, and if it did there would be nothing to distinguish the “intelligence” of the brain from the “intelligence” of the computer simulation. Although this may be impossible in practice, I agree with it. But AI researchers don’t simulate brains, and their programs are not intelligent. You can’t simulate a brain without first understanding what it does.
After my rejection by both Intel and MIT, I didn’t know what to do. When you don’t know how to proceed, often the best strategy is to make no changes until your options become clear. So I just kept working in the computer field. I was content to stay in Boston, but in 1982 my wife wanted to move to California, so we did (it was, again, the path of least friction). I landed a job in Silicon Valley, at a start-up called Grid Systems. Grid invented the laptop computer, a beautiful machine that became the first computer in the collection at the Museum of Modern Art in New York. Working first in marketing and then in engineering, I eventually created a high-level programming language called GridTask. It and I became more and more important to Grid’s success; my career was going well.
Still, I could not get my curiosity about the brain and intelligent machines out of my head. I was consumed with a desire to study brains. So I took a correspondence course in human physiology and studied on my own (no one ever got rejected by a correspondence school!). After learning a fair amount of biology, I decided to apply for admission to a biology graduate program and study intelligence from within the biological sciences. If the computer science world didn’t want a brain theorist, then maybe the biology world would welcome a computer scientist. There was no such thing as theoretical biology back then, and especially not theoretical neuroscience, so biophysics seemed like the best field for my interests. I studied hard, took the required entrance exams, prepared a résumé, solicited letters of recommendation, and voilà, I was accepted as a full-time graduate student in the biophysics program at the University of California at Berkeley.
I was thrilled. Finally I could start in earnest on brain theory, or so I thought. I quit my job at Grid with no intention of working in the computer industry again. Of course this meant indefinitely giving up my salary. My wife was thinking “time to buy a house and start a family” and I was happily becoming a non-provider. This was definitely not a low-friction path. But it was the best option I had, and she supported my decision.
John Ellenby, the founder of Grid, pulled me into his office just before I left and said, “I know you don’t expect to ever come back to Grid or the computer industry, but you never know what will happen. Instead of stopping completely, why don’t you take a leave of absence? That way if, in a year or two, you do come back, you can pick up your salary, position, and stock options where you left off.” It was a nice gesture. I accepted it, but I felt I was leaving the computer business for good.
2
 NEURAL NETWORKS
When I started at UC Berkeley in January 1986, the first thing I did was compile a history of theories of intelligence and brain function. I read hundreds of papers by anatomists, physiologists, philosophers, linguists, computer scientists, and psychologists. Numerous people from many fields had written extensively about thinking and intelligence. Each field had its own set of journals and each used its own terminology. I found their descriptions inconsistent and incomplete. Linguists talked of intelligence in terms such as “syntax” and “semantics.” To them, the brain and intelligence was all about language. Vision scientists referred to 2D, 2½D, and 3D sketches. To them, the brain and intelligence was all about visual pattern recognition. Computer scientists talked of schemas and frames, new terms they made up to represent knowledge. None of these people talked about the structure of the brain and how it would implement any of their theories. On the other hand, anatomists and neurophysiologists wrote extensively about the structure of the brain and how neurons behave, but they mostly avoided any attempt at large-scale theory. It was difficult and frustrating trying to make sense of these various approaches and the mountain of experimental data that accompanied them.
Around this time, a new and promising approach to thinking about intelligent machines burst onto the scene. Neural networks had been around since the late 1960s in one form or another, but neural networks and the AI movement were competitors, for both the dollars and the mind share of the agencies that fund research. AI, the 800-pound gorilla in those days, actively squelched neural network research. Neural network researchers were essentially blacklisted from getting funding for several years. A few people continued to think about them though, and in the mid-1980s their day in the sun had finally arrived. It is hard to know exactly why there was a sudden interest in neural networks, but undoubtedly one contributing factor was the continuing failure of artificial intelligence. People were casting about for alternatives to AI and found one in artificial neural networks.
Neural networks were a genuine improvement over the AI approach because their architecture is based, though very loosely, on real nervous systems. Instead of programming computers, neural network researchers, also known as connectionists, were interested in learning what kinds of behaviors could be exhibited by hooking a bunch of neurons together. Brains are made of neurons; therefore, the brain is a neural network. That is a fact. The hope of connectionists was that the elusive properties of intelligence would become clear by studying how neurons interact, and that some of the problems that were unsolvable with AI could be solved by replicating the correct connections between populations of neurons. A neural network is unlike a computer in that it has no CPU and doesn’t store information in a centralized memory. The network’s knowledge and memories are distributed throughout its connectivity—just like real brains.
On the surface, neural networks seemed to be a great fit with my own interests. But I quickly became disillusioned with the field. By this time I had formed an opinion that three things were essential to understanding the brain. My first criterion was the inclusion of time in brain function. Real brains process rapidly changing streams of information. There is nothing static about the flow of information into and out of the brain.
The second criterion was the importance of feedback. Neuroanatomists have known for a long time that the brain is saturated with feedback connections. For example, in the circuit between the neocortex and a lower structure called the thalamus, connections going backward (toward the input) exceed the connections going forward by almost a factor often! That is, for every fiber feeding information forward into the neocortex, there are ten fibers feeding information back toward the senses. Feedback dominates most connections throughout the neocortex as well. No one understood the precise role of this feedback, but it was clear from published research that it existed everywhere. I figured it must be important.
The third criterion was that any theory or model of the brain should account for the physical architecture of the brain. The neocortex is not a simple structure. As we will see later, it is organized as a repeating hierarchy. Any neural network that didn’t acknowledge this structure was certainly not going to work like a brain.
But as the neural network phenomenon exploded on the scene, it mostly settled on a class of ultrasimple models that didn’t meet any of these criteria. Most neural networks consisted of a small number of neurons connected in three rows. A pattern (the input) is presented to the first row. These input neurons are connected to the next row of neurons, the so-called hidden units. The hidden units then connect to the final row of neurons, the output units. The connections between neurons have variable strengths, meaning the activity in one neuron might increase the activity in another and decrease the activity in a third neuron depending on the connection strengths. By changing these strengths, the network learns to map input patterns to output patterns.
These simple neural networks only processed static patterns, did not use feedback, and didn’t look anything like brains. The most common type of neural network, called a “back propagation” network, learned by broadcasting an error from the output units back toward the input units. You might think this is a form of feedback, but it isn’t really. The backward propagation of errors only occurred during the learning phase. When the neural network was working normally, after being trained, the information flowed only one way. There was no feedback from outputs to inputs. And the models had no sense of time. A static input pattern got converted into a static output pattern. Then another input pattern was presented. There was no history or record in the network of what happened even a short time earlier. And finally the architecture of these neural networks was trivial compared to the complicated and hierarchical structure of the brain.
I thought the field would quickly move on to more realistic networks, but it didn’t. Because these simple neural networks were able to do interesting things, research seemed to stop right there, for years. They had found a new and interesting tool, and overnight thousands of scientists, engineers, and students were getting grants, earning PhDs, and writing books about neural networks. Companies were formed to use neural networks to predict the stock market, process loan applications, verify signatures, and perform hundreds of other pattern classification applications. Although the intent of the founders of the field might have been more general, the field became dominated by people who weren’t interested in understanding how the brain works, or understanding what intelligence is.
The popular press didn’t understand this distinction well.
Newspapers, magazines, and TV science programs presented neural networks as being “brainlike” or working on the “same principles as the brain.” Unlike AI, where everything had to be programmed, neural nets learned by example, which seemed, well, somehow more intelligent. One prominent demonstration was NetTalk. This neural network learned to map sequences of letters onto spoken sounds. As the network was trained on printed text, it started sounding like a computer voice reading the words. It was easy to imagine that, with a little more time, neural networks would be conversing with humans. NetTalk was incorrectly heralded on national news as a machine learning to read. NetTalk was a great exhibition, but what it was actually doing bordered on the trivial. It didn’t read, it didn’t understand, and was of little practical value. It just matched letter combinations to predefined sound patterns.
Let me give you an analogy to show how far neural networks were from real brains. Imagine that instead of trying to figure out how a brain worked we were trying to figure out how a digital computer worked. After years of study, we discover that everything in the computer is made of transistors. There are hundreds of millions of transistors in a computer and they are connected together in precise and complex ways. But we don’t understand how the computer works or why the transistors are connected the way they are. So one day we decide to connect just a few transistors together to see what happens. Lo and behold we find that as few as three transistors, when connected together in a certain way, become an amplifier. A small signal put into one end is magnified on the other end. (Amplifiers in radios and televisions are made using transistors in this fashion.) This is an important discovery, and overnight an industry springs up making transistor radios, televisions, and other electronic appliances using transistor amplifiers. This is all well and good, but it doesn’t tell us anything about how the computer works. Even though an amplifier and a computer are both made of transistors, they have almost nothing else in common. In the same way, a real brain and a three-row neural network are built with neurons, but have almost nothing else in common.
During the summer of 1987, I had an experience that threw more cold water on my already low enthusiasm for neural nets. I went to a neural network conference where I saw a presentation by a company called Nestor. Nestor was trying to sell a neural network application for recognizing handwriting on a tablet. It was offering to license the program for one million dollars. That got my attention. Although Nestor was promoting the sophistication of its neural network algorithm and touting it as yet another major breakthrough, I felt the problem of handwriting recognition could be solved in a simpler, more traditional way. I went home that night, thought about the problem, and in two days had designed a handwriting recognizer that was fast, small, and flexible. My solution didn’t use a neural network and it didn’t work at all like a brain. Although that conference sparked my interest in designing computers with a stylus interface (eventually leading to the PalmPilot ten years later), it also convinced me that neural networks were not much of an improvement over traditional methods. The handwriting recognizer I created ultimately became the basis for the text entry system, called Graffiti, used in the first series of Palm products. I think Nestor went out of business.
So much for simple neural networks. Most of their capabilities were easily handled by other methods and eventually the media hoopla subsided. At least neural network researchers did not claim their models were intelligent. After all, they were extremely simple networks and did less than AI programs. I don’t want to leave you with the impression that all neural networks are of the simple three-layer variety. Some researchers have continued to study neural networks of different designs. Today the term neural network is used to describe a diverse set of models, some of which are more biologically accurate and some of which are not. But almost none of them attempt to capture the overall function or architecture of the neocortex.
In my opinion, the most fundamental problem with most neural networks is a trait they share with AI programs. Both are fatally burdened by their focus on behavior. Whether they are calling these behaviors “answers,” “patterns,” or “outputs,” both AI and neural networks assume intelligence lies in the behavior that a program or a neural network produces after processing a given input. The most important attribute of a computer program or a neural network is whether it gives the correct or desired output. As inspired by Alan Turing, intelligence equals behavior.
But intelligence is not just a matter of acting or behaving intelligently. Behavior is a manifestation of intelligence, but not the central characteristic or primary definition of being intelligent. A moment’s reflection proves this: You can be intelligent just lying in the dark, thinking and understanding. Ignoring what goes on in your head and focusing instead on behavior has been a large impediment to understanding intelligence and building intelligent machines.
Before we explore a new definition of intelligence, I want to tell you about one other connectionist approach that came much closer to describing how real brains work. Trouble is, few people seem to have realized the importance of this research.
While neural nets grabbed the limelight, a small splinter group of neural network theorists built networks that didn’t focus on behavior. Called auto-associative memories, they were also built out of simple “neurons” that connected to each other and fired when they reached a certain threshold. But they were interconnected differently, using lots of feedback. Instead of only passing information forward, as in a back propagation network, auto-associative memories fed the output of each neuron back into the input—sort of like calling yourself on the phone. This feedback loop led to some interesting features. When a pattern of activity was imposed on the artificial neurons, they formed a memory of this pattern. The auto-associative network associated patterns with themselves, hence the term auto-associative memory.
The result of this wiring may at first seem ridiculous. To retrieve a pattern stored in such a memory, you must provide the pattern you want to retrieve. It would be like going to the grocer and asking to buy a bunch of bananas. When the grocer asks you how you will pay, you offer to pay with bananas. What good is that? you might ask. But an auto-associative memory has a few important properties that are found in real brains.
The most important property is that you don’t have to have the entire pattern you want to retrieve in order to retrieve it. You might have only part of the pattern, or you might have a somewhat messed-up pattern. The auto-associative memory can retrieve the correct pattern, as it was originally stored, even though you start with a messy version of it. It would be like going to the grocer with half eaten brown bananas and getting whole green bananas in return. Or going to the bank with a ripped and unreadable bill and the banker says, “I think this is a messed-up $100 bill. Give me that one, and I will give you this new, crisp $100 bill.”
Second, unlike most other neural networks, an autoassociative memory can be designed to store sequences of patterns, or temporal patterns. This feature is accomplished by adding a time delay to the feedback. With this delay, you can present an auto-associative memory with a sequence of patterns, similar to a melody, and it can remember the sequence. I might feed in the first few notes of “Twinkle Twinkle Little Star” and the memory returns the whole song. When presented with part of the sequence, the memory can recall the rest. As we will see later, this is how people learn practically everything, as a sequence of patterns. And I propose the brain uses circuits similar to an auto-associative memory to do so.
Auto-associative memories hinted at the potential importance of feedback and time-changing inputs. But the vast majority of AI, neural network, and cognitive scientists ignored time and feedback.
Neuroscientists as a whole have not done much better. They too know about feedback—they are the people who discovered it—but most have no theory (beyond vague talk of “phases” and “modulation”) to account for why the brain needs to have so much of it. And time has little or no central role in most of their ideas on overall brain function. They tend to chart the brain in terms of where things happen, not when or how neural firing patterns interact over time. Part of this bias comes from the limits of our current experimental techniques. One of the favorite technologies of the 1990s, aka the Decade of the Brain, was functional imaging. Functional imaging machines can take pictures of brain activity in humans. However, they cannot see rapid changes. So scientists ask subjects to concentrate on a single task over and over again as if they were being asked to stand still for an optical photograph, except this is a mental photograph. The result is we have lots of data on where in the brain certain tasks occur, but little data on how realistic, time-varying inputs flow through the brain. Functional imaging lends insight into where things are happening at a given moment but cannot easily capture how brain activity changes over time. Scientists would like to collect this data, but there are few good techniques for doing so. Thus many mainstream cognitive neuroscientists continue to buy into the input-output fallacy. You present a fixed input and see what output you get. Wiring diagrams of the cortex tend to show flowcharts that start in the primary sensory areas where sights, sounds, and touch come in, flow up through higher analytical, planning, and motor areas, and then feed instructions down to the muscles. You sense, then you act.
I don’t want to imply that everyone has ignored time and feedback. This is such a big field that virtually every idea has its adherents. In recent years, belief in the importance of feedback, time, and prediction has been on the rise. But the thunder of AI and classical neural networks kept other approaches subdued and underappreciated for many years.
It’s not difficult to understand why people—laymen and experts alike—have thought that behavior defines intelligence. For at least a couple of centuries people have likened the brain’s abilities to clockworks, then pumps and pipes, then steam engines and, later, to computers. Decades of science fiction have been awash in AI ideas, from Isaac Asimov’s laws of robotics to Star Wars’ C3PO. The idea of intelligent machines doing things is engrained in our imagination. All machines, whether made by humans or imagined by humans, are designed to do something. We don’t have machines that think, we have machines that do. Even as we observe our fellow humans, we focus on their behavior and not on their hidden thoughts. Therefore, it seems intuitively obvious that intelligent behavior should be the metric of an intelligent system.
However, looking across the history of science, we see our intuition is often the biggest obstacle to discovering the truth. Scientific frameworks are often difficult to discover, not because they are complex, but because intuitive but incorrect assumptions keep us from seeing the correct answer. Astronomers before Copernicus (1473-1543) wrongly assumed that the earth was stationary at the center of the universe because it feels stationary and appears to be at the center of the universe. It was intuitively obvious that the stars were all part of a giant spinning sphere, with us at its center. To suggest the Earth was spinning like a top, the surface moving at nearly a thousand miles an hour, and that the entire Earth was hurtling through space—not to mention that stars are trillions of miles away—would have marked you as a lunatic. But that turned out to be the correct framework. Simple to understand, but intuitively incorrect.
Before Darwin (1809-1882), it seemed obvious that species are fixed in their forms. Crocodiles don’t mate with hummingbirds; they are distinct and irreconcilable. The idea that species evolve went against not only religious teachings but also common sense. Evolution implies that you have a common ancestor with every living thing on this planet, including worms and the flowering plant in your kitchen. We now know this to be true, but intuition says otherwise.
I mention these famous examples because I believe that the quest for intelligent machines has also been burdened by an intuitive assumption that’s hampering our progress. When you ask yourself, What does an intelligent system do?, it is intuitively obvious to think in terms of behavior. We demonstrate human intelligence through our speech, writing, and actions, right? Yes, but only to a point. Intelligence is something that is happening in your head. Behavior is an optional ingredient. This is not intuitively obvious, but it’s not hard to understand either.
In the spring of 1986, as I sat at my desk day after day reading scientific articles, building my history of intelligence, and watching the evolving worlds of AI and neural networks, I found myself drowning in details. There was an unending supply of things to study and read about, but I was not gaining any clear understanding of how the whole brain actually worked or even what it did. This was because the field of neuroscience itself was awash in details. It still is. Thousands of research reports are published every year, but they tend to add to the heap rather than organize it. There’s still no overall theory, no framework, explaining what your brain does and how it does it.
I started imagining what the solution to this problem would be like. Is it going to be extremely complex because the brain is so complex? Would it take one hundred pages of dense mathematics to describe how the brain works? Would we need to map out hundreds or thousands of separate circuits before anything useful could be understood? I didn’t think so. History shows that the best solutions to scientific problems are simple and elegant. While the details may be forbidding and the road to a final theory may be arduous, the ultimate conceptual framework is generally simple.
Without a core explanation to guide inquiry, neuroscientists don’t have much to go on as they try to assemble all the details they’ve collected into a coherent picture. The brain is incredibly complex, a vast and daunting tangle of cells. At first glance it looks like a stadium full of cooked spaghetti. It’s also been described as an electrician’s nightmare. But with close and careful inspection we see that the brain isn’t a random heap. It has lots of organization and structure—but much too much of it for us to hope we’ll be able to just intuit the workings of the whole, the way we’re able to see how the shards of a broken vase fit back together. The failing isn’t one of not having enough data or even the right pieces of data; what we need is a shift in perspective. With the proper framework, the details will become meaningful and manageable. Consider the following fanciful analogy to get the flavor of what I mean.
Imagine that millennia from now humans have gone extinct, and explorers from an advanced alien civilization land on Earth. They want to figure out how we lived. They are especially puzzled by our network of roadways. What were these bizarre elaborate structures for? They begin by cataloging everything, both via satellites and from the ground. They are meticulous archaeologists. They record the location of every stray fragment of asphalt, every signpost that has fallen over and been carried downhill by erosion, every detail they can find. They note that some road networks are different from others; in some places they are windy and narrow and almost random-looking, in some places they form a nice regular grid, and over some stretches they become thick and run for hundreds of miles through the desert. They collect a mountain of details, but these details don’t mean anything to them. They continue to collect more detail in hopes of finding some new data that explain it all. They remain stumped for a long time.
That is, until one of them says, “Eureka! I think I see … these creatures couldn’t teleport themselves like we can. They had to travel around from place to place, perhaps on mobile platforms of a cunning design.” From this basic insight, many details begin to fall into place. The small twisty street networks are from early times when the means of conveyance were slow. The thick long roadways were for traveling long distances at high speeds, suggesting at last an explanation for why the signs on those roads had different numbers painted on them. The scientists start to deduce residential versus industrial zoning, the way the needs of commerce and transportation infrastructure might have interacted, and so on. Many of the details they had cataloged turn out to be not very relevant, just accidents of history or the requirements of local geography. The same amount of raw data exists, but it no longer is puzzling.
We can be confident that the same type of breakthrough will let us understand what all the brain’s details are about.
Unfortunately, not everyone believes we can understand how the brain works. A surprising number of people, including a few neuroscientists, believe that somehow the brain and intelligence are beyond explanation. And some believe that even if we could understand them, it would be impossible to build machines that work the same way, that intelligence requires a human body, neurons, and perhaps some new and unfathomable laws of physics. Whenever I hear these arguments, I imagine the intellectuals of the past who argued against studying the heavens or fought against dissecting cadavers to see how our bodies worked. “Don’t bother studying that, it will lead to no good, and even if you could understand how it works there is nothing we can do with that knowledge.” Arguments like this one lead us to a branch of philosophy called functionalism, our last stop in this brief history of our thinking about thinking.
According to functionalism, being intelligent or having a mind is purely a property of organization and has nothing inherently to do with what you’re organized out of. A mind exists in any system whose constituent parts have the right causal relationship with each other, but those parts can just as validly be neurons, silicon chips, or something else. Clearly, this view is standard issue to any would-be builder of intelligent machines.
Consider: Would a game of chess be any less real if it was played with a salt shaker standing in for a lost knight piece? Clearly not. The salt shaker is functionally equivalent to a “real” knight by virtue of how it moves on the board and interacts with the other pieces, so your game is truly a game of chess and not just a simulation of one. Or consider, wouldn’t this sentence be the same if I were to go through it with my cursor deleting each character, then retyping it? Or to take an example closer to home, consider the fact that every few years your body replaces most of the atoms that comprise you. In spite of this, you remain yourself in all the ways that matter to you. One atom is as good as any other if it’s playing the same functional role in your molecular makeup. The same story should hold for the brain: if a mad scientist were to replace each of your neurons with a functionally equivalent micromachine replica, you should come out of the procedure feeling no less your own true self than you had at the outset.
By this principle, an artificial system that used the same functional architecture as an intelligent, living brain should be likewise intelligent—and not just contrivedly so, but actually, truly intelligent.
AI proponents, connectionists, and I are all functionalists, insofar as we all believe there’s nothing inherently special or magical about the brain that allows it to be intelligent. We all believe we’ll be able to build intelligent machines, somehow, someday. But there are different interpretations of functionalism. While I’ve already stated what I consider the central failing of the AI and the connectionist paradigms—the input-output fallacy—there’s a bit more worth saying about why we haven’t yet been able to design intelligent machines. While the AI proponents take what I consider a self-defeating hard line, the connectionists, in my view, have mainly been just too timid.
AI researchers ask, “Why should we engineers be bound by the solutions evolution happened to stumble upon?” In principle, they have a point. Biological systems, like the brain and the genome, are viewed as notoriously inelegant. A common metaphor is that of the Rube Goldberg machine, named after the Depression-era cartoonist who drew comically overcomplicated contraptions to accomplish trivial tasks. Software designers have a related term, kludge, to refer to programs that are written without foresight and wind up full of burdensome, useless complexity, often to the point of becoming incomprehensible even to the programmers who wrote them. AI researchers fear the brain is similarly a mess, a several-hundred-million-year-old kludge, chock-full of inefficiencies and evolutionary “legacy code.” If so, they wonder, why not just throw out the whole sorry clutter and start afresh?
Many philosophers and cognitive psychologists are sympathetic to this view. They love the metaphor of the mind being like software that’s run by the brain, the organic analog of computer hardware. In computers, the hardware level and the software level are distinct from each other. The same software program can be made to run on any Universal Turing Machine. You can run WordPerfect on a PC, a Macintosh, or a Cray supercomputer, for example, even though all three systems have different hardware configurations. And the hardware has nothing of importance to teach you if you’re trying to learn WordPerfect. By analogy, the thinking goes, the brain has nothing to teach us about the mind.
AI defenders also like to point out historical instances in which the engineering solution differs radically from nature’s version. For example, how did we succeed in building flying machines? By imitating the flapping action of winged animals? No. We did it with fixed wings and propellers, and later with jet engines. It may not be how nature did it, but it works—and does so far better than flapping wings.
Similarly, we made a land vehicle that could outrun cheetahs not by making four-legged, cheetah-like running machines, but by inventing wheels. Wheels are a great way to move over flat terrain, and just because evolution never stumbled across that particular strategy doesn’t mean it’s not an excellent way for us to get around. Some philosophers of mind have taken a shine to the metaphor of the “cognitive wheel,” that is, an AI solution to some problem that although entirely different from how the brain does it is just as good. In other words, a program that produces outputs resembling (or surpassing) human performance on a task in some narrow but useful way really is just as good as the way our brains do it.
I believe this kind of ends-justify-the-means interpretation of functionalism leads AI researchers astray. As Searle showed with the Chinese Room, behavioral equivalence is not enough. Since intelligence is an internal property of a brain, we have to look inside the brain to understand what intelligence is. In our investigations of the brain, and especially the neocortex, we will need to be careful in figuring out which details are just superfluous “frozen accidents” of our evolutionary past; undoubtedly, many Rube Goldberg-style processes are mixed in with the important features. But as we’ll soon see, there is an underlying elegance of great power, one that surpasses our best computers, waiting to be extracted from these neural circuits.
Connectionists intuitively felt the brain wasn’t a computer and that its secrets lie in how neurons behave when connected together. That was a good start, but the field barely moved on from its early successes. Although thousands of people worked on three-layer networks, and many still do, research on cortically realistic networks was, and remains, rare.
For half a century we’ve been bringing the full force of our species’ considerable cleverness to trying to program intelligence into computers. In the process we’ve come up with word processors, databases, video games, the Internet, mobile phones, and convincing computer-animated dinosaurs. But intelligent machines still aren’t anywhere in the picture. To succeed, we will need to crib heavily from nature’s engine of intelligence, the neocortex. We have to extract intelligence from within the brain. No other road will get us there.